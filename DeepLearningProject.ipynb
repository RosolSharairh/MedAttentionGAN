{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_VP-rezgMBUl"
   },
   "source": [
    "# **PART 1: CONTRASTIVE LEARNING USING CLIP**\n",
    "In this section, we use the CLIP model to do contrastive learning on our paired CT-MR images, hence obtaining embeddings with better semantic representation, which will be later used as a starting point when doing image-to-image translation between unpaired CT and MR bulk of images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-3KCu2BmO6Pd"
   },
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "UeG820HQMBAz",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.models import *\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms, utils\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from glob import glob\n",
    "from itertools import product\n",
    "from fastai.vision import *\n",
    "from fastai.vision.models import *\n",
    "import glob\n",
    "import argparse\n",
    "import itertools\n",
    "import sys\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "_u88b958bHNq",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# torch.manual_seed(42)\n",
    "\n",
    "BATCH_SIZE=32\n",
    "NUMBER_EPOCHS=5\n",
    "IMG_SIZE=256\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ADnUVXt-O8J0"
   },
   "source": [
    "Defining the generator that will be later used in the Attention-based CycleGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "rxNOi9zUPI1u",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ResnetBlock(nn.Module):\n",
    "    def __init__(self, dim, norm_layer, use_dropout):\n",
    "        super(ResnetBlock, self).__init__()\n",
    "        self.conv_block = self.build_conv_block(dim, norm_layer, use_dropout)\n",
    "\n",
    "    def build_conv_block(self, dim, norm_layer, use_dropout):\n",
    "        conv_block = [nn.ReflectionPad2d(1),\n",
    "                       nn.Conv2d(dim, dim, kernel_size=3),\n",
    "                       norm_layer(dim),\n",
    "                       nn.ReLU(True)]\n",
    "        if use_dropout:\n",
    "            conv_block += [nn.Dropout(0.5)]\n",
    "\n",
    "        conv_block += [nn.ReflectionPad2d(1),\n",
    "                       nn.Conv2d(dim, dim, kernel_size=3),\n",
    "                       norm_layer(dim)]\n",
    "\n",
    "        return nn.Sequential(*conv_block)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x + self.conv_block(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "CFEcbPr57xn8",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_nc=3, output_nc=4, ngf=64, norm_layer=nn.BatchNorm2d, use_dropout=False, n_blocks=6):\n",
    "        assert(n_blocks >= 0)\n",
    "        super(Encoder, self).__init__()\n",
    "        self.input_nc = input_nc\n",
    "        self.output_nc = output_nc\n",
    "        self.ngf = ngf\n",
    "\n",
    "        model = [nn.ReflectionPad2d(3),\n",
    "                 nn.Conv2d(input_nc, ngf, kernel_size=7, padding=0),\n",
    "                 norm_layer(ngf),\n",
    "                 nn.ReLU(True)]\n",
    "\n",
    "        n_downsampling = 2\n",
    "        for i in range(n_downsampling):\n",
    "            mult = 2**i\n",
    "            model += [nn.Conv2d(ngf * mult, ngf * mult * 2, kernel_size=3,\n",
    "                                stride=2, padding=1),\n",
    "                      norm_layer(ngf * mult * 2),\n",
    "                      nn.ReLU(True)]\n",
    "\n",
    "        mult = 2**n_downsampling\n",
    "        for i in range(n_blocks):\n",
    "            model += [ResnetBlock(ngf * mult, norm_layer=norm_layer, use_dropout=use_dropout)]\n",
    "\n",
    "        for i in range(n_downsampling):\n",
    "            mult = 2**(n_downsampling - i)\n",
    "            model += [nn.ReflectionPad2d(1),\n",
    "                      nn.Conv2d(ngf * mult, int(ngf * mult / 2),\n",
    "                                kernel_size=3, stride=1),\n",
    "                      norm_layer(int(ngf * mult / 2)),\n",
    "                      nn.ReLU(True),\n",
    "                      nn.Conv2d(int(ngf * mult / 2), int(ngf * mult / 2)*4,\n",
    "                                kernel_size=1, stride=1),\n",
    "                      nn.PixelShuffle(2),\n",
    "                      norm_layer(int(ngf * mult / 2)),\n",
    "                      nn.ReLU(True),\n",
    "                     ]\n",
    "        model += [nn.ReflectionPad2d(3)]\n",
    "        model += [nn.Conv2d(ngf, output_nc, kernel_size=7, padding=0)]\n",
    "\n",
    "        self.model = nn.Sequential(*model)\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.model(input)\n",
    "        attention_mask = F.sigmoid(output[:, :1])\n",
    "        content_mask = output[:, 1:]\n",
    "        attention_mask = attention_mask.repeat(1, 3, 1, 1)\n",
    "        result = content_mask * attention_mask + input * (1 - attention_mask)\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N0dS-_jQj23p"
   },
   "source": [
    "Defining our customized CLIP network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "YX0BjOTuF_6y",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DepthwiseSeparableConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        super(DepthwiseSeparableConv, self).__init__()\n",
    "        self.depthwise_conv = nn.Conv2d(\n",
    "            in_channels,\n",
    "            in_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=stride,\n",
    "            padding=padding,\n",
    "            groups=in_channels,\n",
    "        )\n",
    "        self.pointwise_conv = nn.Conv2d(\n",
    "            in_channels, out_channels, kernel_size=1, stride=1, padding=0\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.depthwise_conv(x)\n",
    "        x = self.pointwise_conv(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "kEQ4i4ND8pUC",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CLIP(nn.Module):\n",
    "    def __init__(self, image_encoder_1, image_encoder_2):\n",
    "        super(CLIP, self).__init__()\n",
    "        self.image_encoder_1 = image_encoder_1\n",
    "        self.image_encoder_2 = image_encoder_2\n",
    "\n",
    "        self.conv1_1 = DepthwiseSeparableConv(3, 32, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv1_2 = DepthwiseSeparableConv(3, 32, kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.conv2_1 = DepthwiseSeparableConv(32, 64, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv2_2 = DepthwiseSeparableConv(32, 64, kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.pool_1 = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.pool_2 = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "\n",
    "\n",
    "        self.fc_1 = nn.Linear(4096, 1024)\n",
    "        self.fc_2 = nn.Linear(4096, 1024)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, images_1, images_2):\n",
    "        image_encoder_1 = self.image_encoder_1(images_1)\n",
    "        image_encoder_1 = self.pool_1(F.relu(self.conv1_1(image_encoder_1)))\n",
    "        image_encoder_1 = self.pool_1(F.relu(self.conv2_1(image_encoder_1)))\n",
    "        image_encoder_1 = image_encoder_1.view(-1, 4096)\n",
    "        image_embeddings_1 = F.relu(self.fc_1(image_encoder_1))\n",
    "\n",
    "        image_encoder_2 = self.image_encoder_1(images_1)\n",
    "        image_encoder_2 = self.pool_2(F.relu(self.conv1_2(image_encoder_2)))\n",
    "        image_encoder_2 = self.pool_2(F.relu(self.conv2_2(image_encoder_2)))\n",
    "        image_encoder_2 = image_encoder_2.view(-1, 4096)\n",
    "        image_embeddings_2 = F.relu(self.fc_2(image_encoder_2))\n",
    "\n",
    "        return image_embeddings_1, image_embeddings_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gnsHFx-1j6ll"
   },
   "source": [
    "Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "-VSEwq5ibZoH",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def imshow(img, text=None, should_save=False):\n",
    "    npimg = img.numpy()\n",
    "    plt.axis(\"off\")\n",
    "    if text:\n",
    "        plt.text(75, 8, text, style='italic',fontweight='bold',\n",
    "            bbox={'facecolor':'white', 'alpha':0.8, 'pad':10})\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "def show_plot(iteration,loss):\n",
    "    plt.plot(iteration,loss)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lizT32bkkE94"
   },
   "source": [
    "Constructing the data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "rVKHlvL2buc6",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PairedImageDataset(Dataset):\n",
    "    def __init__(self, root_dir_1, root_dir_2, transform=None):\n",
    "        self.root_dir_1 = root_dir_1\n",
    "        self.root_dir_2 = root_dir_2\n",
    "        self.transform = transform\n",
    "        self.image_list_1 = sorted(os.listdir(root_dir_1))\n",
    "        self.image_list_2 = sorted(os.listdir(root_dir_2))\n",
    "        self.all_pairs = list(product(self.image_list_1, self.image_list_2))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name_1, img_name_2 = self.all_pairs[idx]\n",
    "        img_path_1 = os.path.join(self.root_dir_1, img_name_1)\n",
    "        img_path_2 = os.path.join(self.root_dir_2, img_name_2)\n",
    "\n",
    "        image_1 = Image.open(img_path_1).convert(\"RGB\")\n",
    "        image_2 = Image.open(img_path_2).convert(\"RGB\")\n",
    "\n",
    "        if self.transform:\n",
    "            if transforms.RandomHorizontalFlip().p > 0.5:\n",
    "                image1 = transforms.functional.hflip(image_1)\n",
    "                image2 = transforms.functional.hflip(image_2)\n",
    "            image_1 = self.transform(image_1)\n",
    "            image_2 = self.transform(image_2)\n",
    "\n",
    "        label = 1 if img_name_1.replace('CT','') == img_name_2.replace('MR','') else 0\n",
    "\n",
    "\n",
    "        return image_1, image_2, torch.tensor(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 349
    },
    "id": "J61-WSEDkLoN",
    "outputId": "d1388e99-8af4-4103-c522-b602789a51ca",
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/image_CT'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m root_dir_1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/content/drive/MyDrive/image_CT\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      9\u001b[0m root_dir_2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/content/drive/MyDrive/image_MR\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 11\u001b[0m paired_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mPairedImageDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot_dir_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mroot_dir_2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m data_loader \u001b[38;5;241m=\u001b[39m DataLoader(paired_dataset, batch_size\u001b[38;5;241m=\u001b[39mBATCH_SIZE, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[8], line 6\u001b[0m, in \u001b[0;36mPairedImageDataset.__init__\u001b[0;34m(self, root_dir_1, root_dir_2, transform)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot_dir_2 \u001b[38;5;241m=\u001b[39m root_dir_2\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;241m=\u001b[39m transform\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_list_1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot_dir_1\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_list_2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(os\u001b[38;5;241m.\u001b[39mlistdir(root_dir_2))\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_pairs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(product(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_list_1, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_list_2))\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/image_CT'"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    # transforms.CenterCrop(200),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "root_dir_1 = \"/content/drive/MyDrive/image_CT\"\n",
    "root_dir_2 = \"/content/drive/MyDrive/image_MR\"\n",
    "\n",
    "paired_dataset = PairedImageDataset(root_dir_1, root_dir_2, transform=transform)\n",
    "data_loader = DataLoader(paired_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EOuN_uPkkN0m"
   },
   "source": [
    "We use focal loss to address the class imbalance problem of pair labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "HioxdIhSi_UG",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1, gamma=2, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
    "\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return torch.mean(focal_loss)\n",
    "        elif self.reduction == 'sum':\n",
    "            return torch.sum(focal_loss)\n",
    "        elif self.reduction == 'none':\n",
    "            return focal_loss\n",
    "        else:\n",
    "            raise ValueError(\"Invalid reduction option. Use 'mean', 'sum', or 'none'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sW8AdRqkkZCW"
   },
   "source": [
    "Model training (no validation needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "QSKgGtsGkfPx",
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_encoder_1 = Encoder() #ct\n",
    "image_encoder_2 = Encoder() #mr\n",
    "\n",
    "clip_model = CLIP(image_encoder_1, image_encoder_2).to(device)\n",
    "criterion = nn.CosineEmbeddingLoss() #nn.CrossEntropyLoss() #FocalLoss(alpha=1, gamma=2, reduction='mean')\n",
    "optimizer = optim.Adam(clip_model.parameters(), lr=3e-5)\n",
    "\n",
    "counter = []\n",
    "loss_history = []\n",
    "iteration_number= 0\n",
    "NUMBER_EPOCHS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 274
    },
    "id": "NYvJRRPc-6_b",
    "outputId": "8d2d1c86-7ed0-401f-bb4d-e2b9407cf5ad",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 starts.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'data_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(NUMBER_EPOCHS):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m starts.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[43mdata_loader\u001b[49m, \u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m      4\u001b[0m         img_ct, img_mr , labels \u001b[38;5;241m=\u001b[39m data\n\u001b[1;32m      5\u001b[0m         img_ct, img_mr , labels \u001b[38;5;241m=\u001b[39m img_ct\u001b[38;5;241m.\u001b[39mto(device), img_mr\u001b[38;5;241m.\u001b[39mto(device) , labels\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data_loader' is not defined"
     ]
    }
   ],
   "source": [
    "for epoch in range(NUMBER_EPOCHS):\n",
    "    print(f'Epoch {epoch + 1} starts.')\n",
    "    for i, data in enumerate(data_loader, 0):\n",
    "        img_ct, img_mr , labels = data\n",
    "        img_ct, img_mr , labels = img_ct.to(device), img_mr.to(device) , labels.to(device)\n",
    "\n",
    "        image_embedding, text_embedding = clip_model(img_ct, img_mr)\n",
    "\n",
    "        loss = criterion(image_embedding, text_embedding, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        loss_history.append(loss.item())\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{NUMBER_EPOCHS}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZuAtE9DuR7oF"
   },
   "outputs": [],
   "source": [
    "model_path = \"/content/drive/MyDrive/dl_project/CLIP.pth\"\n",
    "\n",
    "torch.save({\n",
    "    'model_state_dict': clip_model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "}, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RUpt9rjgV6dm",
    "outputId": "a92a61be-8af3-421a-a214-b4ec4989ade6",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = \"CLIP.pth\"\n",
    "clip_model.load_state_dict(torch.load(model_path)['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "_6TQmczl5R-9"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/medvqa/juh_mr_ct/mr'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m image_folder \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/drive/MyDrive/medvqa/juh_mr_ct/mr\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 2\u001b[0m image_paths \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimage_folder\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_folder\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[1;32m      4\u001b[0m images \u001b[38;5;241m=\u001b[39m [transform(Image\u001b[38;5;241m.\u001b[39mopen(image_path)\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m\"\u001b[39m))\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m image_path \u001b[38;5;129;01min\u001b[39;00m image_paths]\n\u001b[1;32m      5\u001b[0m image_embeddings \u001b[38;5;241m=\u001b[39m [clip_model(image, image)[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images]\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/medvqa/juh_mr_ct/mr'"
     ]
    }
   ],
   "source": [
    "image_folder = '/content/drive/MyDrive/medvqa/juh_mr_ct/mr'\n",
    "image_paths = [f\"{image_folder}/{img}\" for img in os.listdir(image_folder)]\n",
    "\n",
    "images = [transform(Image.open(image_path).convert(\"RGB\")).unsqueeze(0).to(device) for image_path in image_paths]\n",
    "image_embeddings = [clip_model(image, image)[1] for image in images]\n",
    "image_embeddings = torch.stack(image_embeddings)\n",
    "\n",
    "query_image_path = \"/content/drive/MyDrive/medvqa/juh_mr_ct/ct/10_CT_s1.png\"\n",
    "query_image = transform(Image.open(query_image_path).convert(\"RGB\")).unsqueeze(0).to(device)\n",
    "query_embedding, _ = clip_model(query_image, torch.zeros(1, dtype=torch.long).to(device))\n",
    "\n",
    "similarity_scores = [nn.functional.cosine_similarity(query_embedding, image_embedding, dim=1) for image_embedding in image_embeddings]\n",
    "similarity_scores = torch.stack(similarity_scores)\n",
    "\n",
    "most_similar_index = torch.argmax(similarity_scores).item()\n",
    "most_similar_image_path = image_paths[most_similar_index]\n",
    "\n",
    "print(f\"Most similar image: {most_similar_image_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UZ3BHkov1xp7"
   },
   "source": [
    "# **PART 2: ATTENTIONGAN TO MAP UNPAIRED DOMAINS**\n",
    "We use AttentionGAN-v1 to map unpaired images from separate domains i.e. CT -> MR and MR -> CT. We initialize the weights for the generators of both mappings with the ones learned from the CLIP network.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ab3HDKgaXO8z"
   },
   "source": [
    "Defining utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "XCac5MnQW1vk",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def print_network(net):\n",
    "    num_params = 0\n",
    "    for param in net.parameters():\n",
    "        num_params += param.numel()\n",
    "    print(net)\n",
    "    print('Total number of parameters: %d' % num_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "AeR_kHhCW4pM",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tensor2image(tensor):\n",
    "    image = 127.5*(tensor[0].cpu().float().numpy() + 1.0)\n",
    "    if image.shape[0] == 1:\n",
    "        image = np.tile(image, (3,1,1))\n",
    "    return image.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "MABEEDLLXB6n",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ReplayBuffer():\n",
    "    def __init__(self, max_size=50):\n",
    "        assert (max_size > 0), 'Empty buffer or trying to create a black hole. Be careful.'\n",
    "        self.max_size = max_size\n",
    "        self.data = []\n",
    "\n",
    "    def push_and_pop(self, data):\n",
    "        to_return = []\n",
    "        for element in data.data:\n",
    "            element = torch.unsqueeze(element, 0)\n",
    "            if len(self.data) < self.max_size:\n",
    "                self.data.append(element)\n",
    "                to_return.append(element)\n",
    "            else:\n",
    "                if random.uniform(0,1) > 0.5:\n",
    "                    i = random.randint(0, self.max_size-1)\n",
    "                    to_return.append(self.data[i].clone())\n",
    "                    self.data[i] = element\n",
    "                else:\n",
    "                    to_return.append(element)\n",
    "        return Variable(torch.cat(to_return))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "lMub13HFXDlq",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LambdaLR():\n",
    "    def __init__(self, n_epochs, offset, decay_start_epoch):\n",
    "        assert ((n_epochs - decay_start_epoch) > 0), \"Decay must start before the training session ends!\"\n",
    "        self.n_epochs = n_epochs\n",
    "        self.offset = offset\n",
    "        self.decay_start_epoch = decay_start_epoch\n",
    "\n",
    "    def step(self, epoch):\n",
    "        return 1.0 - max(0, epoch + self.offset - self.decay_start_epoch)/(self.n_epochs - self.decay_start_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "5uo6S6FbXFWg",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def weights_init_normal(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        torch.nn.init.normal(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm2d') != -1:\n",
    "        torch.nn.init.normal(m.weight.data, 1.0, 0.02)\n",
    "        torch.nn.init.constant(m.bias.data, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "wFnw6SYJaBDT",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, root, transforms_=None, unaligned=False, mode='train'):\n",
    "        self.transform = transforms.Compose(transforms_)\n",
    "        self.unaligned = unaligned\n",
    "\n",
    "        self.files_A = os.listdir(os.path.join(root, 'CT'))\n",
    "        self.files_A = [os.path.join(root, 'CT', f) for f in self.files_A]\n",
    "        self.files_B = os.listdir(os.path.join(root, 'MR'))\n",
    "        self.files_B = [os.path.join(root, 'MR', f) for f in self.files_B]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        item_A = self.transform(Image.open(self.files_A[index % len(self.files_A)]))\n",
    "\n",
    "        if self.unaligned:\n",
    "            item_B = self.transform(Image.open(self.files_B[random.randint(0, len(self.files_B) - 1)]))\n",
    "        else:\n",
    "            item_B = self.transform(Image.open(self.files_B[index % len(self.files_B)]))\n",
    "\n",
    "        return {'A': item_A, 'B': item_B}\n",
    "\n",
    "    def __len__(self):\n",
    "        return max(len(self.files_A), len(self.files_B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "pPbw63yZWQsv",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_nc=3, output_nc=4, ngf=64, norm_layer=nn.BatchNorm2d, use_dropout=False, n_blocks=6):\n",
    "        assert(n_blocks >= 0)\n",
    "        super(Generator, self).__init__()\n",
    "        self.input_nc = input_nc\n",
    "        self.output_nc = output_nc\n",
    "        self.ngf = ngf\n",
    "\n",
    "        model = [nn.ReflectionPad2d(3),\n",
    "                 nn.Conv2d(input_nc, ngf, kernel_size=7, padding=0),\n",
    "                 norm_layer(ngf),\n",
    "                 nn.ReLU(True)]\n",
    "\n",
    "        n_downsampling = 2\n",
    "        for i in range(n_downsampling):\n",
    "            mult = 2**i\n",
    "            model += [nn.Conv2d(ngf * mult, ngf * mult * 2, kernel_size=3,\n",
    "                                stride=2, padding=1),\n",
    "                      norm_layer(ngf * mult * 2),\n",
    "                      nn.ReLU(True)]\n",
    "\n",
    "        mult = 2**n_downsampling\n",
    "        for i in range(n_blocks):\n",
    "            model += [ResnetBlock(ngf * mult, norm_layer=norm_layer, use_dropout=use_dropout)]\n",
    "\n",
    "        for i in range(n_downsampling):\n",
    "            mult = 2**(n_downsampling - i)\n",
    "            model += [nn.ReflectionPad2d(1),\n",
    "                      nn.Conv2d(ngf * mult, int(ngf * mult / 2),\n",
    "                                kernel_size=3, stride=1),\n",
    "                      norm_layer(int(ngf * mult / 2)),\n",
    "                      nn.ReLU(True),\n",
    "                      nn.Conv2d(int(ngf * mult / 2), int(ngf * mult / 2)*4,\n",
    "                                kernel_size=1, stride=1),\n",
    "                      nn.PixelShuffle(2),\n",
    "                      norm_layer(int(ngf * mult / 2)),\n",
    "                      nn.ReLU(True),\n",
    "                     ]\n",
    "        model += [nn.ReflectionPad2d(3)]\n",
    "        model += [nn.Conv2d(ngf, output_nc, kernel_size=7, padding=0)]\n",
    "\n",
    "        self.model = nn.Sequential(*model)\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.model(input)\n",
    "        attention_mask = F.sigmoid(output[:, :1])\n",
    "        content_mask = output[:, 1:]\n",
    "        attention_mask = attention_mask.repeat(1, 3, 1, 1)\n",
    "        result = content_mask * attention_mask + input * (1 - attention_mask)\n",
    "\n",
    "        return result, attention_mask, content_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "EY6x0NVlWhBv",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.conv_tower = nn.Sequential(\n",
    "            nn.Conv2d(3,   64,  4, 2),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(64,  128,  4, 2),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(128,  256,  4, 2),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(256,  512, 4, 2),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(512, 512, 4),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            nn.Conv2d(512, 1, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        output = self.conv_tower(img)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "CwslkRqJrpYx",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchmetrics.image import StructuralSimilarityIndexMeasure, PeakSignalNoiseRatio, UniversalImageQualityIndex, VisualInformationFidelity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "1W5dyI3Su5eQ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 931,
     "referenced_widgets": [
      "bcad0ed52b784d6f8c045f987ac08837",
      "b38c39c8506f4e3583e153b62d54aa47",
      "ba3a7b322c5b488c95ca9ababad6b547",
      "b519be0dc5e549a6a18937107657e530",
      "29a6839f6f7b4e2dae8669c903ad11e6",
      "578d1bcbf5aa45e5aeabbab2187bcfca",
      "f1b489b72b564db9b2f557a11378df14",
      "200272c1e0244cf1bac778dc4e2ecc84"
     ]
    },
    "id": "mYo7OQvcu9v4",
    "outputId": "ac561236-8a14-472d-ab51-639b9a24e93e",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:ro6ob3py) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>CT Identity Loss</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>CT Pixel-Wise Reconstruction Loss</td><td>▃▅▁▁▃▄▃▃▃▅▂▅▃▅▂▅▄▄▆▃▄█▄▄▄▇▃▆▃█▃▄▄▄▃▃▃▄▆▅</td></tr><tr><td>CT Reconstruction Loss</td><td>▆▇▃▃▄▄▆▆▅▄▆▅▃▅▄▅▇▄▄▁▃█▃▂▄█▂▂▃▅▃▁▆▂▄▄▄▂█▂</td></tr><tr><td>CT to MR Loss</td><td>▂▁▂▃▆▃▃▇▃▄▃▅▁▅█▃▃▃▅▄▄▇▅▆█▅▄▄▇▆▅▇▅▃▆▅▄▄▃▂</td></tr><tr><td>Discriminator Loss</td><td>▄▆▅▄▂▅▆▂▆▃▃▁▅▁▅▄▄▂▄▃▇▁▂█▄▂█▄▂▂▂▅▆▅▁▂▁▁▂▂</td></tr><tr><td>Generator Loss</td><td>▄▂▃▆▄▃▄▅▃▅▃▆▁▄▅▃▅▄▆▅▅█▄▆▅▇▃▅▇█▅▇▃▄▇▆▅▄▄▃</td></tr><tr><td>MR Identity Loss</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>MR Pixel-Wise Reconstruction Loss</td><td>▇▄▁▄▂▁▅▂▆▂▄▅▃▄▃▃▂▄▄▃▄▄▄█▅▆▄▅▅▅▇▅█▅▆█▆▅▅▇</td></tr><tr><td>MR Reconstruction Loss</td><td>▄█▂▆▁▃▅▇▇▄▇▅▃▅▄▄▃▅▆▃▂▄▅▄▇▆▄▆▇█▄▃▆▃▃▂▅▃▇▆</td></tr><tr><td>MR to CT Loss</td><td>▅▂▄▇▃▃▄▁▂▆▂▆▁▃▂▂▅▃▇▅▆█▄▅▂▇▁▅▆█▃▇▁▄█▇▅▂▃▄</td></tr><tr><td>Peak Signal-to-Noise Ratio (PSNR) - CT to MR</td><td>▃▆▇▄▃▃▂▅▄▇▅▃▃▃▂▃▆▆▄▄▆▂▃▂▁▆▄▂▂▃▅▅▅▅▄▆█▅▄▄</td></tr><tr><td>Peak Signal-to-Noise Ratio (PSNR) - MR to CT</td><td>▂▁▅▆▅▇▄▄▄▅▇▄▅▆▄▇▆▅▄▇▆▅▆▃▆▅▆▇▅▄▃▅▄▇▄▆▅▄▃█</td></tr><tr><td>Structural Similarity Index Measure (SSIM) - CT to MR</td><td>▃▅▆▅▄▆▄▇▅▅█▃▄▄▅▃▄▆▄▆▄▁▇▄▂▅▇▄▅▂▃▅▂▄▄▄▅▃▆▄</td></tr><tr><td>Structural Similarity Index Measure (SSIM) - MR to CT</td><td>▄▁▆▇▆▇▅▄▅▄▄▅▆▅▅▅▆▆▆▇▆█▆▃▇▇█▆█▄▄▄▂▆▅▅▄▆▄▆</td></tr><tr><td>Universal Quality Index (UQI) - CT to MR</td><td>██████ █████▆▇ ███▁█████▆████ ██ ███████</td></tr><tr><td>Universal Quality Index (UQI) - MR to CT</td><td>▆▇▆▆▇▇▇▇█▇█▇▆ ▇▆▇▇ ▁█▆▇▇▇▇▆▆▄▇▇▅█▇▇█▆▆▆▇</td></tr><tr><td>Visual Information Fidelity (VIF) - CT to MR</td><td>▃▃▂▂▄▃█▂▅▂▂▄▃▆▂▃▄▂▃▁▃▂▂▅▆▃▂▂▃▅▁▁▅▁▁▁▂▂▃▁</td></tr><tr><td>Visual Information Fidelity (VIF) - MR to CT</td><td>▅▄▄▄▆▃█▃▃▂▂▃▂▃▆▃▄▃▆▄▃▂▄▁▃▃▁▄▄▁▃▃▄▄▂▆▂▂▂▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>CT Identity Loss</td><td>0.0</td></tr><tr><td>CT Pixel-Wise Reconstruction Loss</td><td>0.56545</td></tr><tr><td>CT Reconstruction Loss</td><td>0.55495</td></tr><tr><td>CT to MR Loss</td><td>0.47967</td></tr><tr><td>Discriminator Loss</td><td>0.02521</td></tr><tr><td>Generator Loss</td><td>0.5953</td></tr><tr><td>MR Identity Loss</td><td>0.0</td></tr><tr><td>MR Pixel-Wise Reconstruction Loss</td><td>0.44827</td></tr><tr><td>MR Reconstruction Loss</td><td>0.42146</td></tr><tr><td>MR to CT Loss</td><td>0.47124</td></tr><tr><td>Peak Signal-to-Noise Ratio (PSNR) - CT to MR</td><td>17.92731</td></tr><tr><td>Peak Signal-to-Noise Ratio (PSNR) - MR to CT</td><td>18.15613</td></tr><tr><td>Structural Similarity Index Measure (SSIM) - CT to MR</td><td>0.40197</td></tr><tr><td>Structural Similarity Index Measure (SSIM) - MR to CT</td><td>0.3792</td></tr><tr><td>Universal Quality Index (UQI) - CT to MR</td><td>-0.03342</td></tr><tr><td>Universal Quality Index (UQI) - MR to CT</td><td>0.00745</td></tr><tr><td>Visual Information Fidelity (VIF) - CT to MR</td><td>0.03228</td></tr><tr><td>Visual Information Fidelity (VIF) - MR to CT</td><td>0.04321</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">deep-plasma-25</strong> at: <a href='https://wandb.ai/thedlproject/deep_learning_project/runs/ro6ob3py' target=\"_blank\">https://wandb.ai/thedlproject/deep_learning_project/runs/ro6ob3py</a><br/> View job at <a href='https://wandb.ai/thedlproject/deep_learning_project/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjEyNjkwOTYzMA==/version_details/v4' target=\"_blank\">https://wandb.ai/thedlproject/deep_learning_project/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjEyNjkwOTYzMA==/version_details/v4</a><br/>Synced 6 W&B file(s), 180 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240103_132358-ro6ob3py/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:ro6ob3py). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9524754271d241ee92b989213b720c2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016669731796719135, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/ibex/user/sabbam0a/template/dlp/wandb/run-20240103_145013-jr2eqc9o</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thedlproject/deep_learning_project/runs/jr2eqc9o' target=\"_blank\">trim-surf-26</a></strong> to <a href='https://wandb.ai/thedlproject/deep_learning_project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thedlproject/deep_learning_project' target=\"_blank\">https://wandb.ai/thedlproject/deep_learning_project</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thedlproject/deep_learning_project/runs/jr2eqc9o' target=\"_blank\">https://wandb.ai/thedlproject/deep_learning_project/runs/jr2eqc9o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/thedlproject/deep_learning_project/runs/jr2eqc9o?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x147e39a66cd0>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project='deep_learning_project',\n",
    "    entity='thedlproject'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "7ixM2fVm489O",
    "tags": []
   },
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "epoch = 0 #starting epoch\n",
    "n_epochs = 10 #number of epochs of training\n",
    "batch_size = 1 #size of the batches\n",
    "dataroot = 'train' # '/content/drive/MyDrive/DL Project/Data/unpaired line 7 /train' #root directory of the dataset\n",
    "save_name = 'MedAttentionGAN_2_no_clip'\n",
    "lr = 0.0001 #initial learning rate\n",
    "decay_epoch = 8 #epoch to start linearly decaying the learning rate to 0\n",
    "size = 256 #size of the data crop (squared assumed)\n",
    "input_nc = 3 #number of channels of input data\n",
    "output_nc = 3 #number of channels of output data\n",
    "cuda = 'store_true' #use GPU computation\n",
    "n_cpu = 8 #number of cpu threads to use during batch generation\n",
    "lambda_cycle = 10\n",
    "lambda_identity = 0\n",
    "lambda_pixel = 1\n",
    "lambda_reg = 1e-6\n",
    "\n",
    "gan_curriculum = 10  #Strong GAN loss for certain period at the beginning\n",
    "starting_rate = 0.01 #Set the lambda weight between GAN loss and Recon loss during curriculum period at the beginning. We used the 0.01 weight.\n",
    "default_rate = 0.5 #Set the lambda weight between GAN loss and Recon loss after curriculum period. We used the 0.5 weight.\n",
    "\n",
    "\n",
    "if torch.cuda.is_available() and not cuda:\n",
    "    print(\"No CUDA!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "prL5P3rbYM7a",
    "outputId": "507171a6-77fe-44ea-a8c4-2c7b6ea4bc4b",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = \"CLIP.pth\" #\"/content/drive/MyDrive/DL Project/CLIP.pth\"\n",
    "clip_model.load_state_dict(torch.load(model_path)['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F9aUGSnK5BCy",
    "outputId": "e1e6c510-78f1-4e39-9c1b-dfc431bf332d",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Networks initialized -------------\n",
      "Generator(\n",
      "  (model): Sequential(\n",
      "    (0): ReflectionPad2d((3, 3, 3, 3))\n",
      "    (1): Conv2d(3, 64, kernel_size=(7, 7), stride=(1, 1))\n",
      "    (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (8): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): ResnetBlock(\n",
      "      (conv_block): Sequential(\n",
      "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (3): ReLU(inplace=True)\n",
      "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (11): ResnetBlock(\n",
      "      (conv_block): Sequential(\n",
      "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (3): ReLU(inplace=True)\n",
      "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (12): ResnetBlock(\n",
      "      (conv_block): Sequential(\n",
      "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (3): ReLU(inplace=True)\n",
      "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (13): ResnetBlock(\n",
      "      (conv_block): Sequential(\n",
      "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (3): ReLU(inplace=True)\n",
      "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (14): ResnetBlock(\n",
      "      (conv_block): Sequential(\n",
      "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (3): ReLU(inplace=True)\n",
      "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (15): ResnetBlock(\n",
      "      (conv_block): Sequential(\n",
      "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (3): ReLU(inplace=True)\n",
      "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (16): ReflectionPad2d((1, 1, 1, 1))\n",
      "    (17): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (18): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): ReLU(inplace=True)\n",
      "    (20): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (21): PixelShuffle(upscale_factor=2)\n",
      "    (22): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (23): ReLU(inplace=True)\n",
      "    (24): ReflectionPad2d((1, 1, 1, 1))\n",
      "    (25): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (26): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (27): ReLU(inplace=True)\n",
      "    (28): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (29): PixelShuffle(upscale_factor=2)\n",
      "    (30): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (31): ReLU(inplace=True)\n",
      "    (32): ReflectionPad2d((3, 3, 3, 3))\n",
      "    (33): Conv2d(64, 4, kernel_size=(7, 7), stride=(1, 1))\n",
      "  )\n",
      ")\n",
      "Total number of parameters: 7931332\n",
      "Generator(\n",
      "  (model): Sequential(\n",
      "    (0): ReflectionPad2d((3, 3, 3, 3))\n",
      "    (1): Conv2d(3, 64, kernel_size=(7, 7), stride=(1, 1))\n",
      "    (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (8): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): ResnetBlock(\n",
      "      (conv_block): Sequential(\n",
      "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (3): ReLU(inplace=True)\n",
      "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (11): ResnetBlock(\n",
      "      (conv_block): Sequential(\n",
      "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (3): ReLU(inplace=True)\n",
      "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (12): ResnetBlock(\n",
      "      (conv_block): Sequential(\n",
      "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (3): ReLU(inplace=True)\n",
      "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (13): ResnetBlock(\n",
      "      (conv_block): Sequential(\n",
      "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (3): ReLU(inplace=True)\n",
      "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (14): ResnetBlock(\n",
      "      (conv_block): Sequential(\n",
      "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (3): ReLU(inplace=True)\n",
      "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (15): ResnetBlock(\n",
      "      (conv_block): Sequential(\n",
      "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (3): ReLU(inplace=True)\n",
      "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (16): ReflectionPad2d((1, 1, 1, 1))\n",
      "    (17): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (18): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): ReLU(inplace=True)\n",
      "    (20): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (21): PixelShuffle(upscale_factor=2)\n",
      "    (22): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (23): ReLU(inplace=True)\n",
      "    (24): ReflectionPad2d((1, 1, 1, 1))\n",
      "    (25): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (26): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (27): ReLU(inplace=True)\n",
      "    (28): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (29): PixelShuffle(upscale_factor=2)\n",
      "    (30): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (31): ReLU(inplace=True)\n",
      "    (32): ReflectionPad2d((3, 3, 3, 3))\n",
      "    (33): Conv2d(64, 4, kernel_size=(7, 7), stride=(1, 1))\n",
      "  )\n",
      ")\n",
      "Total number of parameters: 7931332\n",
      "Discriminator(\n",
      "  (conv_tower): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.01)\n",
      "    (3): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2))\n",
      "    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): LeakyReLU(negative_slope=0.01)\n",
      "    (6): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2))\n",
      "    (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (8): LeakyReLU(negative_slope=0.01)\n",
      "    (9): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2))\n",
      "    (10): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (11): LeakyReLU(negative_slope=0.01)\n",
      "    (12): Conv2d(512, 512, kernel_size=(4, 4), stride=(1, 1))\n",
      "    (13): LeakyReLU(negative_slope=0.01)\n",
      "    (14): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "    (15): Conv2d(512, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      ")\n",
      "Total number of parameters: 6953793\n",
      "Discriminator(\n",
      "  (conv_tower): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.01)\n",
      "    (3): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2))\n",
      "    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): LeakyReLU(negative_slope=0.01)\n",
      "    (6): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2))\n",
      "    (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (8): LeakyReLU(negative_slope=0.01)\n",
      "    (9): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2))\n",
      "    (10): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (11): LeakyReLU(negative_slope=0.01)\n",
      "    (12): Conv2d(512, 512, kernel_size=(4, 4), stride=(1, 1))\n",
      "    (13): LeakyReLU(negative_slope=0.01)\n",
      "    (14): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "    (15): Conv2d(512, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      ")\n",
      "Total number of parameters: 6953793\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3105388/303252567.py:4: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n",
      "  torch.nn.init.normal(m.weight.data, 0.0, 0.02)\n",
      "/tmp/ipykernel_3105388/303252567.py:6: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n",
      "  torch.nn.init.normal(m.weight.data, 1.0, 0.02)\n",
      "/tmp/ipykernel_3105388/303252567.py:7: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n",
      "  torch.nn.init.constant(m.bias.data, 0.0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Discriminator(\n",
       "  (conv_tower): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): LeakyReLU(negative_slope=0.01)\n",
       "    (3): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2))\n",
       "    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): LeakyReLU(negative_slope=0.01)\n",
       "    (6): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2))\n",
       "    (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (8): LeakyReLU(negative_slope=0.01)\n",
       "    (9): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2))\n",
       "    (10): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (11): LeakyReLU(negative_slope=0.01)\n",
       "    (12): Conv2d(512, 512, kernel_size=(4, 4), stride=(1, 1))\n",
       "    (13): LeakyReLU(negative_slope=0.01)\n",
       "    (14): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (15): Conv2d(512, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###### Definition of variables ######\n",
    "# Networks\n",
    "netG_A2B = Generator()\n",
    "netG_B2A = Generator()\n",
    "netG_A2B.load_state_dict(clip_model.image_encoder_1.state_dict()) # load from CLIP, ct encoder\n",
    "netG_B2A.load_state_dict(clip_model.image_encoder_2.state_dict()) # load from CLIP, mr encoder\n",
    "\n",
    "netD_A = Discriminator()\n",
    "netD_B = Discriminator()\n",
    "\n",
    "print('---------- Networks initialized -------------')\n",
    "print_network(netG_A2B)\n",
    "print_network(netG_B2A)\n",
    "print_network(netD_A)\n",
    "print_network(netD_B)\n",
    "print('-----------------------------------------------')\n",
    "\n",
    "if cuda:\n",
    "    netG_A2B.cuda()\n",
    "    netG_B2A.cuda()\n",
    "    netD_A.cuda()\n",
    "    netD_B.cuda()\n",
    "\n",
    "# netG_A2B.apply(weights_init_normal)\n",
    "# netG_B2A.apply(weights_init_normal)\n",
    "netD_A.apply(weights_init_normal)\n",
    "netD_B.apply(weights_init_normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "WPsXAypE5Eqv",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Losses\n",
    "criterion_GAN = torch.nn.MSELoss()\n",
    "criterion_cycle = torch.nn.L1Loss()\n",
    "criterion_identity = torch.nn.L1Loss()\n",
    "\n",
    "# Metrics\n",
    "ssi =  StructuralSimilarityIndexMeasure()\n",
    "psnr = PeakSignalNoiseRatio()\n",
    "uqi = UniversalImageQualityIndex()\n",
    "vif = VisualInformationFidelity()\n",
    "# c_ssi = 0\n",
    "# c_psnr = 0\n",
    "# c_uqi = 0\n",
    "# c_vif = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "DKyU3h8k5F0n",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Optimizers & LR schedulers\n",
    "optimizer_G = torch.optim.Adam(itertools.chain(netG_A2B.parameters(), netG_B2A.parameters()),\n",
    "                                lr=lr, betas=(0.5, 0.999))\n",
    "optimizer_D = torch.optim.Adam(itertools.chain(netD_A.parameters(), netD_B.parameters()),\n",
    "                                lr=lr, betas=(0.5, 0.999))\n",
    "\n",
    "lr_scheduler_G = torch.optim.lr_scheduler.LambdaLR(optimizer_G, lr_lambda=LambdaLR(n_epochs, epoch, decay_epoch).step)\n",
    "lr_scheduler_D = torch.optim.lr_scheduler.LambdaLR(optimizer_D, lr_lambda=LambdaLR(n_epochs, epoch, decay_epoch).step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "Qbvk0omHzURv",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Inputs & targets memory allocation\n",
    "Tensor = torch.cuda.FloatTensor if cuda else torch.Tensor\n",
    "input_A = Tensor(batch_size, input_nc, size, size)\n",
    "input_B = Tensor(batch_size, output_nc, size, size)\n",
    "target_real = Variable(Tensor(batch_size).fill_(1.0), requires_grad=False)\n",
    "target_fake = Variable(Tensor(batch_size).fill_(0.0), requires_grad=False)\n",
    "\n",
    "fake_A_buffer = ReplayBuffer()\n",
    "fake_B_buffer = ReplayBuffer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "HFCa2Tfy8bme",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "class GrayscaleToRGB:\n",
    "    \"\"\"Convert a grayscale image to RGB by replicating the single channel.\"\"\"\n",
    "    def __call__(self, img):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img (PIL Image): Grayscale image.\n",
    "\n",
    "        Returns:\n",
    "            PIL Image: RGB image.\n",
    "        \"\"\"\n",
    "        return img.convert('RGB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "dyGu-6nd5Mg6",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ibex/user/sabbam0a/template/env/lib/python3.8/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 6, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    }
   ],
   "source": [
    "# Dataset loader\n",
    "transforms_ = [ GrayscaleToRGB(),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.5,0.5,0.5], [0.5,0.5,0.5])\n",
    "                 ]\n",
    "\n",
    "dataloader = DataLoader(ImageDataset(dataroot, transforms_=transforms_, unaligned=True),\n",
    "                        batch_size=batch_size, shuffle=True, num_workers=n_cpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "hFS-5XHu5J3q",
    "outputId": "99106a6d-fdfc-4440-8af2-c8568f4629be",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ibex/user/sabbam0a/template/env/lib/python3.8/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1, 1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Batch [1/1749], loss_D: 0.3500, loss_G: 0.8336\n",
      "loss_GAN_A2B: 0.3765, loss_GAN_B2A: 0.9333, loss_cycle_ABA: 0.4317, loss_cycle_BAB: 0.6396, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.3040, loss_pix_B: 0.4387\n",
      "Epoch [1/10], Batch [101/1749], loss_D: 0.1091, loss_G: 0.5013\n",
      "loss_GAN_A2B: 0.5851, loss_GAN_B2A: 0.2504, loss_cycle_ABA: 0.2159, loss_cycle_BAB: 0.3016, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.1890, loss_pix_B: 0.3437\n",
      "Epoch [1/10], Batch [201/1749], loss_D: 0.0557, loss_G: 0.7424\n",
      "loss_GAN_A2B: 0.7384, loss_GAN_B2A: 0.5390, loss_cycle_ABA: 0.3617, loss_cycle_BAB: 0.4324, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.3631, loss_pix_B: 0.4227\n",
      "Epoch [1/10], Batch [301/1749], loss_D: 0.0898, loss_G: 0.5898\n",
      "loss_GAN_A2B: 0.6392, loss_GAN_B2A: 0.3644, loss_cycle_ABA: 0.4206, loss_cycle_BAB: 0.2378, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.4987, loss_pix_B: 0.4382\n",
      "Epoch [1/10], Batch [401/1749], loss_D: 0.2378, loss_G: 0.4777\n",
      "loss_GAN_A2B: 0.2897, loss_GAN_B2A: 0.5024, loss_cycle_ABA: 0.3502, loss_cycle_BAB: 0.2804, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.1719, loss_pix_B: 0.3920\n",
      "Epoch [1/10], Batch [501/1749], loss_D: 0.0614, loss_G: 0.6878\n",
      "loss_GAN_A2B: 0.8056, loss_GAN_B2A: 0.3525, loss_cycle_ABA: 0.4464, loss_cycle_BAB: 0.4492, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.4777, loss_pix_B: 0.3786\n",
      "Epoch [1/10], Batch [601/1749], loss_D: 0.0998, loss_G: 0.5559\n",
      "loss_GAN_A2B: 0.6064, loss_GAN_B2A: 0.3508, loss_cycle_ABA: 0.3149, loss_cycle_BAB: 0.2755, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.2943, loss_pix_B: 0.3433\n",
      "Epoch [1/10], Batch [701/1749], loss_D: 0.1228, loss_G: 0.2824\n",
      "loss_GAN_A2B: 0.2764, loss_GAN_B2A: 0.1240, loss_cycle_ABA: 0.3284, loss_cycle_BAB: 0.3020, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.3789, loss_pix_B: 0.2789\n",
      "Epoch [1/10], Batch [801/1749], loss_D: 0.1229, loss_G: 0.5462\n",
      "loss_GAN_A2B: 0.6863, loss_GAN_B2A: 0.1880, loss_cycle_ABA: 0.3969, loss_cycle_BAB: 0.4817, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.5681, loss_pix_B: 0.3778\n",
      "Epoch [1/10], Batch [901/1749], loss_D: 0.0505, loss_G: 0.6164\n",
      "loss_GAN_A2B: 0.7541, loss_GAN_B2A: 0.2119, loss_cycle_ABA: 0.6842, loss_cycle_BAB: 0.4156, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.6392, loss_pix_B: 0.3858\n",
      "Epoch [1/10], Batch [1001/1749], loss_D: 0.0563, loss_G: 0.4652\n",
      "loss_GAN_A2B: 0.5760, loss_GAN_B2A: 0.1491, loss_cycle_ABA: 0.2955, loss_cycle_BAB: 0.5075, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.6585, loss_pix_B: 0.2850\n",
      "Epoch [1/10], Batch [1101/1749], loss_D: 0.1030, loss_G: 0.2572\n",
      "loss_GAN_A2B: 0.1337, loss_GAN_B2A: 0.2207, loss_cycle_ABA: 0.2685, loss_cycle_BAB: 0.3325, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.2894, loss_pix_B: 0.2617\n",
      "Epoch [1/10], Batch [1201/1749], loss_D: 0.3200, loss_G: 0.4601\n",
      "loss_GAN_A2B: 0.6230, loss_GAN_B2A: 0.0020, loss_cycle_ABA: 0.4420, loss_cycle_BAB: 0.7267, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.7350, loss_pix_B: 0.4044\n",
      "Epoch [1/10], Batch [1301/1749], loss_D: 0.0634, loss_G: 0.9011\n",
      "loss_GAN_A2B: 0.7856, loss_GAN_B2A: 0.7909, loss_cycle_ABA: 0.4507, loss_cycle_BAB: 0.4382, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.6932, loss_pix_B: 0.2883\n",
      "Epoch [1/10], Batch [1401/1749], loss_D: 0.1023, loss_G: 0.4756\n",
      "loss_GAN_A2B: 0.4644, loss_GAN_B2A: 0.2830, loss_cycle_ABA: 0.3284, loss_cycle_BAB: 0.5223, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.2871, loss_pix_B: 0.3502\n",
      "Epoch [1/10], Batch [1501/1749], loss_D: 0.0241, loss_G: 0.5995\n",
      "loss_GAN_A2B: 0.1258, loss_GAN_B2A: 0.9269, loss_cycle_ABA: 0.3973, loss_cycle_BAB: 0.1884, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.4735, loss_pix_B: 0.2359\n",
      "Epoch [1/10], Batch [1601/1749], loss_D: 0.1577, loss_G: 0.3652\n",
      "loss_GAN_A2B: 0.2663, loss_GAN_B2A: 0.2879, loss_cycle_ABA: 0.2836, loss_cycle_BAB: 0.4034, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.3639, loss_pix_B: 0.2819\n",
      "Epoch [1/10], Batch [1701/1749], loss_D: 0.3064, loss_G: 0.4541\n",
      "loss_GAN_A2B: 0.6957, loss_GAN_B2A: 0.0016, loss_cycle_ABA: 0.3418, loss_cycle_BAB: 0.4912, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.4076, loss_pix_B: 0.4024\n",
      "Epoch [2/10], Batch [1/1749], loss_D: 0.1225, loss_G: 0.3890\n",
      "loss_GAN_A2B: 0.3026, loss_GAN_B2A: 0.2396, loss_cycle_ABA: 0.4472, loss_cycle_BAB: 0.4822, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.6481, loss_pix_B: 0.2933\n",
      "Epoch [2/10], Batch [101/1749], loss_D: 0.1067, loss_G: 0.8332\n",
      "loss_GAN_A2B: 1.1429, loss_GAN_B2A: 0.3252, loss_cycle_ABA: 0.3583, loss_cycle_BAB: 0.5058, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.3056, loss_pix_B: 0.3023\n",
      "Epoch [2/10], Batch [201/1749], loss_D: 0.0959, loss_G: 0.3774\n",
      "loss_GAN_A2B: 0.5512, loss_GAN_B2A: 0.0094, loss_cycle_ABA: 0.3264, loss_cycle_BAB: 0.4366, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.3497, loss_pix_B: 0.3572\n",
      "Epoch [2/10], Batch [301/1749], loss_D: 0.2469, loss_G: 0.6401\n",
      "loss_GAN_A2B: 1.0751, loss_GAN_B2A: 0.0258, loss_cycle_ABA: 0.3298, loss_cycle_BAB: 0.3832, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.4201, loss_pix_B: 0.2795\n",
      "Epoch [2/10], Batch [401/1749], loss_D: 0.0572, loss_G: 0.3957\n",
      "loss_GAN_A2B: 0.3131, loss_GAN_B2A: 0.2745, loss_cycle_ABA: 0.2907, loss_cycle_BAB: 0.5521, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.3410, loss_pix_B: 0.2673\n",
      "Epoch [2/10], Batch [501/1749], loss_D: 0.0577, loss_G: 0.9019\n",
      "loss_GAN_A2B: 1.0946, loss_GAN_B2A: 0.5186, loss_cycle_ABA: 0.3345, loss_cycle_BAB: 0.4909, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.4170, loss_pix_B: 0.2070\n",
      "Epoch [2/10], Batch [601/1749], loss_D: 0.1304, loss_G: 0.6713\n",
      "loss_GAN_A2B: 0.6167, loss_GAN_B2A: 0.4953, loss_cycle_ABA: 0.4604, loss_cycle_BAB: 0.5457, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.3891, loss_pix_B: 0.3401\n",
      "Epoch [2/10], Batch [701/1749], loss_D: 0.0332, loss_G: 0.9350\n",
      "loss_GAN_A2B: 0.9792, loss_GAN_B2A: 0.6487, loss_cycle_ABA: 0.5413, loss_cycle_BAB: 0.4570, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.7383, loss_pix_B: 0.2333\n",
      "Epoch [2/10], Batch [801/1749], loss_D: 0.1143, loss_G: 0.7647\n",
      "loss_GAN_A2B: 0.9660, loss_GAN_B2A: 0.3652, loss_cycle_ABA: 0.2862, loss_cycle_BAB: 0.5449, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.3135, loss_pix_B: 0.3702\n",
      "Epoch [2/10], Batch [901/1749], loss_D: 0.1667, loss_G: 0.6162\n",
      "loss_GAN_A2B: 0.3159, loss_GAN_B2A: 0.7300, loss_cycle_ABA: 0.2974, loss_cycle_BAB: 0.4371, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.4662, loss_pix_B: 0.3292\n",
      "Epoch [2/10], Batch [1001/1749], loss_D: 0.0631, loss_G: 0.9797\n",
      "loss_GAN_A2B: 0.7803, loss_GAN_B2A: 0.9367, loss_cycle_ABA: 0.4371, loss_cycle_BAB: 0.5861, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.6805, loss_pix_B: 0.3338\n",
      "Epoch [2/10], Batch [1101/1749], loss_D: 0.0207, loss_G: 0.7049\n",
      "loss_GAN_A2B: 1.0254, loss_GAN_B2A: 0.1982, loss_cycle_ABA: 0.3487, loss_cycle_BAB: 0.4258, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.3499, loss_pix_B: 0.3438\n",
      "Epoch [2/10], Batch [1201/1749], loss_D: 0.0709, loss_G: 0.5028\n",
      "loss_GAN_A2B: 0.6405, loss_GAN_B2A: 0.1946, loss_cycle_ABA: 0.4035, loss_cycle_BAB: 0.2334, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.4005, loss_pix_B: 0.3334\n",
      "Epoch [2/10], Batch [1301/1749], loss_D: 0.2679, loss_G: 0.5045\n",
      "loss_GAN_A2B: 0.4649, loss_GAN_B2A: 0.3516, loss_cycle_ABA: 0.4299, loss_cycle_BAB: 0.3336, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.5252, loss_pix_B: 0.3454\n",
      "Epoch [2/10], Batch [1401/1749], loss_D: 0.0525, loss_G: 0.7734\n",
      "loss_GAN_A2B: 0.8321, loss_GAN_B2A: 0.5449, loss_cycle_ABA: 0.3083, loss_cycle_BAB: 0.3863, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.2744, loss_pix_B: 0.3174\n",
      "Epoch [2/10], Batch [1501/1749], loss_D: 0.1104, loss_G: 0.4045\n",
      "loss_GAN_A2B: 0.3401, loss_GAN_B2A: 0.2502, loss_cycle_ABA: 0.2597, loss_cycle_BAB: 0.6005, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.3074, loss_pix_B: 0.2771\n",
      "Epoch [2/10], Batch [1601/1749], loss_D: 0.1014, loss_G: 0.6772\n",
      "loss_GAN_A2B: 0.9079, loss_GAN_B2A: 0.2749, loss_cycle_ABA: 0.2231, loss_cycle_BAB: 0.4972, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.3309, loss_pix_B: 0.2444\n",
      "Epoch [2/10], Batch [1701/1749], loss_D: 0.0328, loss_G: 0.6173\n",
      "loss_GAN_A2B: 0.4593, loss_GAN_B2A: 0.5305, loss_cycle_ABA: 0.3915, loss_cycle_BAB: 0.6392, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.6229, loss_pix_B: 0.4282\n",
      "Epoch [3/10], Batch [1/1749], loss_D: 0.0570, loss_G: 0.6425\n",
      "loss_GAN_A2B: 0.7864, loss_GAN_B2A: 0.3228, loss_cycle_ABA: 0.3421, loss_cycle_BAB: 0.3249, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.3738, loss_pix_B: 0.2916\n",
      "Epoch [3/10], Batch [101/1749], loss_D: 0.1475, loss_G: 0.4222\n",
      "loss_GAN_A2B: 0.5638, loss_GAN_B2A: 0.1296, loss_cycle_ABA: 0.2804, loss_cycle_BAB: 0.2653, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.3042, loss_pix_B: 0.2418\n",
      "Epoch [3/10], Batch [201/1749], loss_D: 0.0894, loss_G: 0.2593\n",
      "loss_GAN_A2B: 0.1718, loss_GAN_B2A: 0.1878, loss_cycle_ABA: 0.3215, loss_cycle_BAB: 0.2834, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.3496, loss_pix_B: 0.3599\n",
      "Epoch [3/10], Batch [301/1749], loss_D: 0.0213, loss_G: 0.7592\n",
      "loss_GAN_A2B: 0.9985, loss_GAN_B2A: 0.3400, loss_cycle_ABA: 0.3535, loss_cycle_BAB: 0.3548, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.4300, loss_pix_B: 0.3665\n",
      "Epoch [3/10], Batch [401/1749], loss_D: 0.1569, loss_G: 0.5155\n",
      "loss_GAN_A2B: 0.7340, loss_GAN_B2A: 0.1346, loss_cycle_ABA: 0.3675, loss_cycle_BAB: 0.2651, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.4146, loss_pix_B: 0.2821\n",
      "Epoch [3/10], Batch [501/1749], loss_D: 0.1076, loss_G: 0.6994\n",
      "loss_GAN_A2B: 0.6240, loss_GAN_B2A: 0.5261, loss_cycle_ABA: 0.4845, loss_cycle_BAB: 0.5576, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.6690, loss_pix_B: 0.5729\n",
      "Epoch [3/10], Batch [601/1749], loss_D: 0.0566, loss_G: 0.9496\n",
      "loss_GAN_A2B: 1.0739, loss_GAN_B2A: 0.6528, loss_cycle_ABA: 0.3373, loss_cycle_BAB: 0.4133, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.2974, loss_pix_B: 0.3247\n",
      "Epoch [3/10], Batch [701/1749], loss_D: 0.1018, loss_G: 0.4622\n",
      "loss_GAN_A2B: 0.5067, loss_GAN_B2A: 0.1783, loss_cycle_ABA: 0.5920, loss_cycle_BAB: 0.3645, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.4504, loss_pix_B: 0.3139\n",
      "Epoch [3/10], Batch [801/1749], loss_D: 0.0676, loss_G: 0.9627\n",
      "loss_GAN_A2B: 0.8153, loss_GAN_B2A: 0.9231, loss_cycle_ABA: 0.4325, loss_cycle_BAB: 0.3051, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.5106, loss_pix_B: 0.3481\n",
      "Epoch [3/10], Batch [901/1749], loss_D: 0.2275, loss_G: 0.5481\n",
      "loss_GAN_A2B: 0.3858, loss_GAN_B2A: 0.5115, loss_cycle_ABA: 0.3905, loss_cycle_BAB: 0.3860, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.3972, loss_pix_B: 0.4130\n",
      "Epoch [3/10], Batch [1001/1749], loss_D: 0.1471, loss_G: 0.5875\n",
      "loss_GAN_A2B: 0.8055, loss_GAN_B2A: 0.2078, loss_cycle_ABA: 0.3224, loss_cycle_BAB: 0.3028, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.2934, loss_pix_B: 0.2718\n",
      "Epoch [3/10], Batch [1101/1749], loss_D: 0.0847, loss_G: 1.1248\n",
      "loss_GAN_A2B: 0.6188, loss_GAN_B2A: 1.4412, loss_cycle_ABA: 0.3872, loss_cycle_BAB: 0.4249, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.2369, loss_pix_B: 0.5176\n",
      "Epoch [3/10], Batch [1201/1749], loss_D: 0.0291, loss_G: 0.8340\n",
      "loss_GAN_A2B: 0.4469, loss_GAN_B2A: 1.0290, loss_cycle_ABA: 0.3389, loss_cycle_BAB: 0.4497, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.3778, loss_pix_B: 0.3334\n",
      "Epoch [3/10], Batch [1301/1749], loss_D: 0.1683, loss_G: 0.9400\n",
      "loss_GAN_A2B: 0.7814, loss_GAN_B2A: 0.9108, loss_cycle_ABA: 0.3289, loss_cycle_BAB: 0.3911, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.3315, loss_pix_B: 0.3800\n",
      "Epoch [3/10], Batch [1401/1749], loss_D: 0.1033, loss_G: 0.4687\n",
      "loss_GAN_A2B: 0.4858, loss_GAN_B2A: 0.2863, loss_cycle_ABA: 0.3759, loss_cycle_BAB: 0.2198, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.3283, loss_pix_B: 0.4080\n",
      "Epoch [3/10], Batch [1501/1749], loss_D: 0.0046, loss_G: 0.8225\n",
      "loss_GAN_A2B: 1.1517, loss_GAN_B2A: 0.2566, loss_cycle_ABA: 0.6158, loss_cycle_BAB: 0.3812, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.6288, loss_pix_B: 0.4418\n",
      "Epoch [3/10], Batch [1601/1749], loss_D: 0.1062, loss_G: 0.6647\n",
      "loss_GAN_A2B: 0.9667, loss_GAN_B2A: 0.1841, loss_cycle_ABA: 0.3922, loss_cycle_BAB: 0.3231, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.5891, loss_pix_B: 0.3014\n",
      "Epoch [3/10], Batch [1701/1749], loss_D: 0.0345, loss_G: 0.6024\n",
      "loss_GAN_A2B: 0.5902, loss_GAN_B2A: 0.4279, loss_cycle_ABA: 0.3060, loss_cycle_BAB: 0.3761, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.4540, loss_pix_B: 0.3083\n",
      "Epoch [4/10], Batch [1/1749], loss_D: 0.0449, loss_G: 0.7922\n",
      "loss_GAN_A2B: 0.8716, loss_GAN_B2A: 0.5042, loss_cycle_ABA: 0.3408, loss_cycle_BAB: 0.5183, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.4242, loss_pix_B: 0.4169\n",
      "Epoch [4/10], Batch [101/1749], loss_D: 0.0536, loss_G: 0.7307\n",
      "loss_GAN_A2B: 0.9730, loss_GAN_B2A: 0.2988, loss_cycle_ABA: 0.4114, loss_cycle_BAB: 0.3833, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.2469, loss_pix_B: 0.3667\n",
      "Epoch [4/10], Batch [201/1749], loss_D: 0.0087, loss_G: 1.0871\n",
      "loss_GAN_A2B: 1.2755, loss_GAN_B2A: 0.6719, loss_cycle_ABA: 0.5324, loss_cycle_BAB: 0.4495, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.5621, loss_pix_B: 0.4285\n",
      "Epoch [4/10], Batch [301/1749], loss_D: 0.0481, loss_G: 0.7834\n",
      "loss_GAN_A2B: 0.6119, loss_GAN_B2A: 0.8041, loss_cycle_ABA: 0.3310, loss_cycle_BAB: 0.2746, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.3787, loss_pix_B: 0.4705\n",
      "Epoch [4/10], Batch [401/1749], loss_D: 0.1406, loss_G: 0.6276\n",
      "loss_GAN_A2B: 0.3773, loss_GAN_B2A: 0.6956, loss_cycle_ABA: 0.3754, loss_cycle_BAB: 0.3238, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.4148, loss_pix_B: 0.4455\n",
      "Epoch [4/10], Batch [501/1749], loss_D: 0.0615, loss_G: 0.6377\n",
      "loss_GAN_A2B: 0.4340, loss_GAN_B2A: 0.7005, loss_cycle_ABA: 0.2290, loss_cycle_BAB: 0.2964, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.3287, loss_pix_B: 0.3642\n",
      "Epoch [4/10], Batch [601/1749], loss_D: 0.0199, loss_G: 0.7963\n",
      "loss_GAN_A2B: 0.6962, loss_GAN_B2A: 0.6654, loss_cycle_ABA: 0.4428, loss_cycle_BAB: 0.4762, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.5941, loss_pix_B: 0.4585\n",
      "Epoch [4/10], Batch [701/1749], loss_D: 0.0550, loss_G: 0.4615\n",
      "loss_GAN_A2B: 0.5023, loss_GAN_B2A: 0.2543, loss_cycle_ABA: 0.2862, loss_cycle_BAB: 0.3307, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.4805, loss_pix_B: 0.3775\n",
      "Epoch [4/10], Batch [801/1749], loss_D: 0.0157, loss_G: 0.5749\n",
      "loss_GAN_A2B: 0.6989, loss_GAN_B2A: 0.3103, loss_cycle_ABA: 0.2509, loss_cycle_BAB: 0.2749, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.4447, loss_pix_B: 0.4101\n",
      "Epoch [4/10], Batch [901/1749], loss_D: 0.1222, loss_G: 0.6839\n",
      "loss_GAN_A2B: 0.5706, loss_GAN_B2A: 0.6207, loss_cycle_ABA: 0.3385, loss_cycle_BAB: 0.3550, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.4278, loss_pix_B: 0.3773\n",
      "Epoch [4/10], Batch [1001/1749], loss_D: 0.1038, loss_G: 0.2289\n",
      "loss_GAN_A2B: 0.1397, loss_GAN_B2A: 0.1128, loss_cycle_ABA: 0.3553, loss_cycle_BAB: 0.3970, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.4448, loss_pix_B: 0.3576\n",
      "Epoch [4/10], Batch [1101/1749], loss_D: 0.2379, loss_G: 0.7383\n",
      "loss_GAN_A2B: 1.0003, loss_GAN_B2A: 0.2920, loss_cycle_ABA: 0.2848, loss_cycle_BAB: 0.4594, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.3227, loss_pix_B: 0.5463\n",
      "Epoch [4/10], Batch [1201/1749], loss_D: 0.0439, loss_G: 0.6191\n",
      "loss_GAN_A2B: 0.5826, loss_GAN_B2A: 0.4972, loss_cycle_ABA: 0.3731, loss_cycle_BAB: 0.2428, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.3921, loss_pix_B: 0.3541\n",
      "Epoch [4/10], Batch [1301/1749], loss_D: 0.0388, loss_G: 0.3739\n",
      "loss_GAN_A2B: 0.1516, loss_GAN_B2A: 0.4332, loss_cycle_ABA: 0.2839, loss_cycle_BAB: 0.2978, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.4788, loss_pix_B: 0.4301\n",
      "Epoch [4/10], Batch [1401/1749], loss_D: 0.1772, loss_G: 0.5308\n",
      "loss_GAN_A2B: 0.7032, loss_GAN_B2A: 0.1851, loss_cycle_ABA: 0.3405, loss_cycle_BAB: 0.2856, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.4369, loss_pix_B: 0.4168\n",
      "Epoch [4/10], Batch [1501/1749], loss_D: 0.1104, loss_G: 1.0720\n",
      "loss_GAN_A2B: 0.7406, loss_GAN_B2A: 1.1720, loss_cycle_ABA: 0.3858, loss_cycle_BAB: 0.5799, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.6279, loss_pix_B: 0.3528\n",
      "Epoch [4/10], Batch [1601/1749], loss_D: 0.0430, loss_G: 0.9628\n",
      "loss_GAN_A2B: 0.8801, loss_GAN_B2A: 0.8030, loss_cycle_ABA: 0.3628, loss_cycle_BAB: 0.6175, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.6511, loss_pix_B: 0.3545\n",
      "Epoch [4/10], Batch [1701/1749], loss_D: 0.1069, loss_G: 0.4236\n",
      "loss_GAN_A2B: 0.2820, loss_GAN_B2A: 0.3389, loss_cycle_ABA: 0.3742, loss_cycle_BAB: 0.5060, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.4559, loss_pix_B: 0.2831\n",
      "Epoch [5/10], Batch [1/1749], loss_D: 0.1026, loss_G: 0.6554\n",
      "loss_GAN_A2B: 0.3845, loss_GAN_B2A: 0.6755, loss_cycle_ABA: 0.2414, loss_cycle_BAB: 0.7450, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.8189, loss_pix_B: 0.3358\n",
      "Epoch [5/10], Batch [101/1749], loss_D: 0.1122, loss_G: 0.9795\n",
      "loss_GAN_A2B: 0.7581, loss_GAN_B2A: 0.9430, loss_cycle_ABA: 0.3808, loss_cycle_BAB: 0.6303, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.7937, loss_pix_B: 0.5046\n",
      "Epoch [5/10], Batch [201/1749], loss_D: 0.1240, loss_G: 0.8709\n",
      "loss_GAN_A2B: 0.5997, loss_GAN_B2A: 0.9258, loss_cycle_ABA: 0.3943, loss_cycle_BAB: 0.5118, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.3943, loss_pix_B: 0.5213\n",
      "Epoch [5/10], Batch [301/1749], loss_D: 0.1058, loss_G: 0.5759\n",
      "loss_GAN_A2B: 0.2759, loss_GAN_B2A: 0.6581, loss_cycle_ABA: 0.3984, loss_cycle_BAB: 0.4691, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.4868, loss_pix_B: 0.4573\n",
      "Epoch [5/10], Batch [401/1749], loss_D: 0.0781, loss_G: 1.0232\n",
      "loss_GAN_A2B: 1.0161, loss_GAN_B2A: 0.8075, loss_cycle_ABA: 0.3562, loss_cycle_BAB: 0.5397, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.6018, loss_pix_B: 0.4823\n",
      "Epoch [5/10], Batch [501/1749], loss_D: 0.0575, loss_G: 0.6234\n",
      "loss_GAN_A2B: 0.7387, loss_GAN_B2A: 0.3151, loss_cycle_ABA: 0.1933, loss_cycle_BAB: 0.5635, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.5906, loss_pix_B: 0.4337\n",
      "Epoch [5/10], Batch [601/1749], loss_D: 0.1098, loss_G: 0.5486\n",
      "loss_GAN_A2B: 0.7085, loss_GAN_B2A: 0.2031, loss_cycle_ABA: 0.3465, loss_cycle_BAB: 0.3799, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.5390, loss_pix_B: 0.4048\n",
      "Epoch [5/10], Batch [701/1749], loss_D: 0.0382, loss_G: 0.5990\n",
      "loss_GAN_A2B: 0.5152, loss_GAN_B2A: 0.5141, loss_cycle_ABA: 0.1827, loss_cycle_BAB: 0.4632, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.6592, loss_pix_B: 0.4467\n",
      "Epoch [5/10], Batch [801/1749], loss_D: 0.0542, loss_G: 0.6213\n",
      "loss_GAN_A2B: 0.7861, loss_GAN_B2A: 0.2566, loss_cycle_ABA: 0.2522, loss_cycle_BAB: 0.4939, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.5510, loss_pix_B: 0.4002\n",
      "Epoch [5/10], Batch [901/1749], loss_D: 0.0278, loss_G: 0.8949\n",
      "loss_GAN_A2B: 1.0909, loss_GAN_B2A: 0.5170, loss_cycle_ABA: 0.3495, loss_cycle_BAB: 0.3763, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.4596, loss_pix_B: 0.5318\n",
      "Epoch [5/10], Batch [1001/1749], loss_D: 0.0313, loss_G: 0.5558\n",
      "loss_GAN_A2B: 0.1668, loss_GAN_B2A: 0.7226, loss_cycle_ABA: 0.3675, loss_cycle_BAB: 0.4915, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.7234, loss_pix_B: 0.5683\n",
      "Epoch [5/10], Batch [1101/1749], loss_D: 0.3117, loss_G: 0.4601\n",
      "loss_GAN_A2B: 0.7186, loss_GAN_B2A: 0.0004, loss_cycle_ABA: 0.2117, loss_cycle_BAB: 0.5733, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.5907, loss_pix_B: 0.5814\n",
      "Epoch [5/10], Batch [1201/1749], loss_D: 0.0382, loss_G: 0.9361\n",
      "loss_GAN_A2B: 0.6244, loss_GAN_B2A: 1.0623, loss_cycle_ABA: 0.4344, loss_cycle_BAB: 0.2909, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.5816, loss_pix_B: 0.7341\n",
      "Epoch [5/10], Batch [1301/1749], loss_D: 0.1252, loss_G: 0.6662\n",
      "loss_GAN_A2B: 0.6816, loss_GAN_B2A: 0.4639, loss_cycle_ABA: 0.2354, loss_cycle_BAB: 0.4790, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.5021, loss_pix_B: 0.5514\n",
      "Epoch [5/10], Batch [1401/1749], loss_D: 0.2632, loss_G: 0.4435\n",
      "loss_GAN_A2B: 0.0230, loss_GAN_B2A: 0.6916, loss_cycle_ABA: 0.1927, loss_cycle_BAB: 0.4255, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.4882, loss_pix_B: 0.5113\n",
      "Epoch [5/10], Batch [1501/1749], loss_D: 0.1520, loss_G: 0.9284\n",
      "loss_GAN_A2B: 0.5039, loss_GAN_B2A: 1.1957, loss_cycle_ABA: 0.1904, loss_cycle_BAB: 0.4315, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.4858, loss_pix_B: 0.4931\n",
      "Epoch [5/10], Batch [1601/1749], loss_D: 0.0319, loss_G: 0.8017\n",
      "loss_GAN_A2B: 0.4810, loss_GAN_B2A: 0.9009, loss_cycle_ABA: 0.3644, loss_cycle_BAB: 0.5596, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.5957, loss_pix_B: 0.6922\n",
      "Epoch [5/10], Batch [1701/1749], loss_D: 0.1835, loss_G: 0.4420\n",
      "loss_GAN_A2B: 0.4090, loss_GAN_B2A: 0.2627, loss_cycle_ABA: 0.1824, loss_cycle_BAB: 0.6652, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.4699, loss_pix_B: 0.4648\n",
      "Epoch [6/10], Batch [1/1749], loss_D: 0.0893, loss_G: 0.7336\n",
      "loss_GAN_A2B: 0.9719, loss_GAN_B2A: 0.2812, loss_cycle_ABA: 0.2495, loss_cycle_BAB: 0.5592, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.5597, loss_pix_B: 0.6452\n",
      "Epoch [6/10], Batch [101/1749], loss_D: 0.0448, loss_G: 0.6410\n",
      "loss_GAN_A2B: 0.6108, loss_GAN_B2A: 0.5073, loss_cycle_ABA: 0.1871, loss_cycle_BAB: 0.4360, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.4256, loss_pix_B: 0.5072\n",
      "Epoch [6/10], Batch [201/1749], loss_D: 0.1066, loss_G: 0.4763\n",
      "loss_GAN_A2B: 0.3050, loss_GAN_B2A: 0.4377, loss_cycle_ABA: 0.2385, loss_cycle_BAB: 0.5541, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.6852, loss_pix_B: 0.5767\n",
      "Epoch [6/10], Batch [301/1749], loss_D: 0.0497, loss_G: 0.7881\n",
      "loss_GAN_A2B: 0.8330, loss_GAN_B2A: 0.5921, loss_cycle_ABA: 0.2916, loss_cycle_BAB: 0.2894, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.4833, loss_pix_B: 0.5290\n",
      "Epoch [6/10], Batch [401/1749], loss_D: 0.2295, loss_G: 0.8180\n",
      "loss_GAN_A2B: 0.5651, loss_GAN_B2A: 0.9003, loss_cycle_ABA: 0.2002, loss_cycle_BAB: 0.4445, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.6184, loss_pix_B: 0.6234\n",
      "Epoch [6/10], Batch [501/1749], loss_D: 0.0845, loss_G: 0.4305\n",
      "loss_GAN_A2B: 0.3185, loss_GAN_B2A: 0.3818, loss_cycle_ABA: 0.2096, loss_cycle_BAB: 0.4079, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.3557, loss_pix_B: 0.6256\n",
      "Epoch [6/10], Batch [601/1749], loss_D: 0.1926, loss_G: 0.5452\n",
      "loss_GAN_A2B: 0.7807, loss_GAN_B2A: 0.0817, loss_cycle_ABA: 0.2525, loss_cycle_BAB: 0.6540, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.6566, loss_pix_B: 0.5519\n",
      "Epoch [6/10], Batch [701/1749], loss_D: 0.0642, loss_G: 0.5950\n",
      "loss_GAN_A2B: 0.4162, loss_GAN_B2A: 0.5908, loss_cycle_ABA: 0.1583, loss_cycle_BAB: 0.5661, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.5475, loss_pix_B: 0.5251\n",
      "Epoch [6/10], Batch [801/1749], loss_D: 0.1173, loss_G: 0.5254\n",
      "loss_GAN_A2B: 0.6058, loss_GAN_B2A: 0.2260, loss_cycle_ABA: 0.4526, loss_cycle_BAB: 0.4178, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.6724, loss_pix_B: 0.4879\n",
      "Epoch [6/10], Batch [901/1749], loss_D: 0.0580, loss_G: 0.7859\n",
      "loss_GAN_A2B: 0.4864, loss_GAN_B2A: 0.8956, loss_cycle_ABA: 0.2131, loss_cycle_BAB: 0.5018, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.5580, loss_pix_B: 0.4449\n",
      "Epoch [6/10], Batch [1001/1749], loss_D: 0.1731, loss_G: 0.6084\n",
      "loss_GAN_A2B: 0.2382, loss_GAN_B2A: 0.8148, loss_cycle_ABA: 0.2262, loss_cycle_BAB: 0.3782, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.6769, loss_pix_B: 0.3922\n",
      "Epoch [6/10], Batch [1101/1749], loss_D: 0.0741, loss_G: 0.6792\n",
      "loss_GAN_A2B: 0.3328, loss_GAN_B2A: 0.8704, loss_cycle_ABA: 0.1809, loss_cycle_BAB: 0.3969, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.4626, loss_pix_B: 0.4769\n",
      "Epoch [6/10], Batch [1201/1749], loss_D: 0.1011, loss_G: 0.6535\n",
      "loss_GAN_A2B: 0.5252, loss_GAN_B2A: 0.5478, loss_cycle_ABA: 0.2643, loss_cycle_BAB: 0.6609, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.5973, loss_pix_B: 0.4761\n",
      "Epoch [6/10], Batch [1301/1749], loss_D: 0.0310, loss_G: 0.7698\n",
      "loss_GAN_A2B: 0.9439, loss_GAN_B2A: 0.3230, loss_cycle_ABA: 0.4260, loss_cycle_BAB: 0.7058, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.6821, loss_pix_B: 0.4236\n",
      "Epoch [6/10], Batch [1401/1749], loss_D: 0.0138, loss_G: 0.7850\n",
      "loss_GAN_A2B: 0.5244, loss_GAN_B2A: 0.8688, loss_cycle_ABA: 0.1814, loss_cycle_BAB: 0.5005, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.4795, loss_pix_B: 0.3795\n",
      "Epoch [6/10], Batch [1501/1749], loss_D: 0.0852, loss_G: 0.8445\n",
      "loss_GAN_A2B: 1.0013, loss_GAN_B2A: 0.4667, loss_cycle_ABA: 0.3749, loss_cycle_BAB: 0.5016, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.4808, loss_pix_B: 0.5069\n",
      "Epoch [6/10], Batch [1601/1749], loss_D: 0.0942, loss_G: 0.7156\n",
      "loss_GAN_A2B: 0.7669, loss_GAN_B2A: 0.4544, loss_cycle_ABA: 0.2419, loss_cycle_BAB: 0.5496, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.6340, loss_pix_B: 0.5122\n",
      "Epoch [6/10], Batch [1701/1749], loss_D: 0.0032, loss_G: 0.5849\n",
      "loss_GAN_A2B: 0.4751, loss_GAN_B2A: 0.5117, loss_cycle_ABA: 0.2436, loss_cycle_BAB: 0.4480, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.5399, loss_pix_B: 0.5031\n",
      "Epoch [7/10], Batch [1/1749], loss_D: 0.1211, loss_G: 0.7081\n",
      "loss_GAN_A2B: 0.2127, loss_GAN_B2A: 1.0351, loss_cycle_ABA: 0.2003, loss_cycle_BAB: 0.4593, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.5133, loss_pix_B: 0.3597\n",
      "Epoch [7/10], Batch [101/1749], loss_D: 0.1271, loss_G: 0.7728\n",
      "loss_GAN_A2B: 0.8452, loss_GAN_B2A: 0.5824, loss_cycle_ABA: 0.1868, loss_cycle_BAB: 0.2891, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.3697, loss_pix_B: 0.3275\n",
      "Epoch [7/10], Batch [201/1749], loss_D: 0.0547, loss_G: 0.6515\n",
      "loss_GAN_A2B: 0.5708, loss_GAN_B2A: 0.5299, loss_cycle_ABA: 0.3862, loss_cycle_BAB: 0.4450, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.4116, loss_pix_B: 0.4912\n",
      "Epoch [7/10], Batch [301/1749], loss_D: 0.0254, loss_G: 0.7617\n",
      "loss_GAN_A2B: 0.6288, loss_GAN_B2A: 0.6772, loss_cycle_ABA: 0.3952, loss_cycle_BAB: 0.5302, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.4930, loss_pix_B: 0.4419\n",
      "Epoch [7/10], Batch [401/1749], loss_D: 0.0117, loss_G: 0.9125\n",
      "loss_GAN_A2B: 0.4109, loss_GAN_B2A: 1.2585, loss_cycle_ABA: 0.2531, loss_cycle_BAB: 0.3565, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.4894, loss_pix_B: 0.4184\n",
      "Epoch [7/10], Batch [501/1749], loss_D: 0.1812, loss_G: 0.5201\n",
      "loss_GAN_A2B: 0.4294, loss_GAN_B2A: 0.3884, loss_cycle_ABA: 0.3988, loss_cycle_BAB: 0.5029, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.4897, loss_pix_B: 0.4413\n",
      "Epoch [7/10], Batch [601/1749], loss_D: 0.0423, loss_G: 0.6745\n",
      "loss_GAN_A2B: 0.9658, loss_GAN_B2A: 0.1505, loss_cycle_ABA: 0.4258, loss_cycle_BAB: 0.5374, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.6245, loss_pix_B: 0.3846\n",
      "Epoch [7/10], Batch [701/1749], loss_D: 0.0143, loss_G: 1.1819\n",
      "loss_GAN_A2B: 0.8402, loss_GAN_B2A: 1.3466, loss_cycle_ABA: 0.2353, loss_cycle_BAB: 0.4808, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.7076, loss_pix_B: 0.4389\n",
      "Epoch [7/10], Batch [801/1749], loss_D: 0.0228, loss_G: 0.7437\n",
      "loss_GAN_A2B: 1.0509, loss_GAN_B2A: 0.1815, loss_cycle_ABA: 0.3240, loss_cycle_BAB: 0.7254, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.7415, loss_pix_B: 0.3738\n",
      "Epoch [7/10], Batch [901/1749], loss_D: 0.0534, loss_G: 0.9317\n",
      "loss_GAN_A2B: 1.1094, loss_GAN_B2A: 0.5500, loss_cycle_ABA: 0.2012, loss_cycle_BAB: 0.5928, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.6459, loss_pix_B: 0.4301\n",
      "Epoch [7/10], Batch [1001/1749], loss_D: 0.0930, loss_G: 0.8055\n",
      "loss_GAN_A2B: 0.8911, loss_GAN_B2A: 0.5435, loss_cycle_ABA: 0.2611, loss_cycle_BAB: 0.4792, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.3802, loss_pix_B: 0.4257\n",
      "Epoch [7/10], Batch [1101/1749], loss_D: 0.0886, loss_G: 1.0410\n",
      "loss_GAN_A2B: 1.1005, loss_GAN_B2A: 0.8201, loss_cycle_ABA: 0.2726, loss_cycle_BAB: 0.3806, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.5040, loss_pix_B: 0.5630\n",
      "Epoch [7/10], Batch [1201/1749], loss_D: 0.0807, loss_G: 0.7197\n",
      "loss_GAN_A2B: 1.0270, loss_GAN_B2A: 0.2321, loss_cycle_ABA: 0.3583, loss_cycle_BAB: 0.3853, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.4025, loss_pix_B: 0.5909\n",
      "Epoch [7/10], Batch [1301/1749], loss_D: 0.1453, loss_G: 0.8283\n",
      "loss_GAN_A2B: 1.1064, loss_GAN_B2A: 0.4171, loss_cycle_ABA: 0.1692, loss_cycle_BAB: 0.3610, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.4964, loss_pix_B: 0.5565\n",
      "Epoch [7/10], Batch [1401/1749], loss_D: 0.0313, loss_G: 1.0953\n",
      "loss_GAN_A2B: 0.8774, loss_GAN_B2A: 1.1284, loss_cycle_ABA: 0.3306, loss_cycle_BAB: 0.4305, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.6008, loss_pix_B: 0.4730\n",
      "Epoch [7/10], Batch [1501/1749], loss_D: 0.0457, loss_G: 0.6520\n",
      "loss_GAN_A2B: 0.5511, loss_GAN_B2A: 0.5844, loss_cycle_ABA: 0.1536, loss_cycle_BAB: 0.4353, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.4518, loss_pix_B: 0.3774\n",
      "Epoch [7/10], Batch [1601/1749], loss_D: 0.1223, loss_G: 0.4161\n",
      "loss_GAN_A2B: 0.3040, loss_GAN_B2A: 0.3424, loss_cycle_ABA: 0.3543, loss_cycle_BAB: 0.3515, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.4696, loss_pix_B: 0.5966\n",
      "Epoch [7/10], Batch [1701/1749], loss_D: 0.0104, loss_G: 0.9268\n",
      "loss_GAN_A2B: 0.8822, loss_GAN_B2A: 0.8160, loss_cycle_ABA: 0.2417, loss_cycle_BAB: 0.3579, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.4131, loss_pix_B: 0.4197\n",
      "Epoch [8/10], Batch [1/1749], loss_D: 0.0069, loss_G: 0.7677\n",
      "loss_GAN_A2B: 0.6941, loss_GAN_B2A: 0.7080, loss_cycle_ABA: 0.2228, loss_cycle_BAB: 0.2835, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.3919, loss_pix_B: 0.2900\n",
      "Epoch [8/10], Batch [101/1749], loss_D: 0.0646, loss_G: 0.8507\n",
      "loss_GAN_A2B: 0.4875, loss_GAN_B2A: 1.0191, loss_cycle_ABA: 0.2733, loss_cycle_BAB: 0.5478, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.5207, loss_pix_B: 0.6460\n",
      "Epoch [8/10], Batch [201/1749], loss_D: 0.0115, loss_G: 1.3374\n",
      "loss_GAN_A2B: 1.3327, loss_GAN_B2A: 1.1253, loss_cycle_ABA: 0.5773, loss_cycle_BAB: 0.3618, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.6240, loss_pix_B: 0.4967\n",
      "Epoch [8/10], Batch [301/1749], loss_D: 0.0416, loss_G: 0.5505\n",
      "loss_GAN_A2B: 0.5782, loss_GAN_B2A: 0.3551, loss_cycle_ABA: 0.2767, loss_cycle_BAB: 0.3292, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.5311, loss_pix_B: 0.4963\n",
      "Epoch [8/10], Batch [401/1749], loss_D: 0.0747, loss_G: 0.8635\n",
      "loss_GAN_A2B: 0.4472, loss_GAN_B2A: 1.1167, loss_cycle_ABA: 0.2189, loss_cycle_BAB: 0.4423, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.4081, loss_pix_B: 0.5341\n",
      "Epoch [8/10], Batch [501/1749], loss_D: 0.2338, loss_G: 0.6060\n",
      "loss_GAN_A2B: 0.3217, loss_GAN_B2A: 0.7432, loss_cycle_ABA: 0.2078, loss_cycle_BAB: 0.3724, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.4233, loss_pix_B: 0.3013\n",
      "Epoch [8/10], Batch [601/1749], loss_D: 0.0077, loss_G: 0.6714\n",
      "loss_GAN_A2B: 0.2252, loss_GAN_B2A: 0.9583, loss_cycle_ABA: 0.2635, loss_cycle_BAB: 0.3666, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.5786, loss_pix_B: 0.5299\n",
      "Epoch [8/10], Batch [701/1749], loss_D: 0.1737, loss_G: 0.8253\n",
      "loss_GAN_A2B: 0.3827, loss_GAN_B2A: 1.1094, loss_cycle_ABA: 0.3241, loss_cycle_BAB: 0.3007, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.5699, loss_pix_B: 0.5550\n",
      "Epoch [8/10], Batch [801/1749], loss_D: 0.0224, loss_G: 1.0843\n",
      "loss_GAN_A2B: 0.8735, loss_GAN_B2A: 1.1109, loss_cycle_ABA: 0.2753, loss_cycle_BAB: 0.4975, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.4877, loss_pix_B: 0.4115\n",
      "Epoch [8/10], Batch [901/1749], loss_D: 0.1270, loss_G: 0.7259\n",
      "loss_GAN_A2B: 0.8504, loss_GAN_B2A: 0.4107, loss_cycle_ABA: 0.2032, loss_cycle_BAB: 0.5505, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.7094, loss_pix_B: 0.4469\n",
      "Epoch [8/10], Batch [1001/1749], loss_D: 0.0249, loss_G: 0.7389\n",
      "loss_GAN_A2B: 0.3880, loss_GAN_B2A: 0.9542, loss_cycle_ABA: 0.1641, loss_cycle_BAB: 0.3552, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.5278, loss_pix_B: 0.3176\n",
      "Epoch [8/10], Batch [1101/1749], loss_D: 0.0724, loss_G: 0.7148\n",
      "loss_GAN_A2B: 0.7180, loss_GAN_B2A: 0.4842, loss_cycle_ABA: 0.2712, loss_cycle_BAB: 0.6589, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.6544, loss_pix_B: 0.5834\n",
      "Epoch [8/10], Batch [1201/1749], loss_D: 0.1616, loss_G: 0.4807\n",
      "loss_GAN_A2B: 0.3332, loss_GAN_B2A: 0.4611, loss_cycle_ABA: 0.2853, loss_cycle_BAB: 0.3508, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.5044, loss_pix_B: 0.4573\n",
      "Epoch [8/10], Batch [1301/1749], loss_D: 0.0192, loss_G: 0.9013\n",
      "loss_GAN_A2B: 0.7469, loss_GAN_B2A: 0.8826, loss_cycle_ABA: 0.2489, loss_cycle_BAB: 0.4782, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.5971, loss_pix_B: 0.4939\n",
      "Epoch [8/10], Batch [1401/1749], loss_D: 0.0536, loss_G: 0.6602\n",
      "loss_GAN_A2B: 0.6233, loss_GAN_B2A: 0.4869, loss_cycle_ABA: 0.2728, loss_cycle_BAB: 0.5692, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.6409, loss_pix_B: 0.6000\n",
      "Epoch [8/10], Batch [1501/1749], loss_D: 0.0980, loss_G: 0.7708\n",
      "loss_GAN_A2B: 0.7034, loss_GAN_B2A: 0.6571, loss_cycle_ABA: 0.2947, loss_cycle_BAB: 0.4216, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.5203, loss_pix_B: 0.5111\n",
      "Epoch [8/10], Batch [1601/1749], loss_D: 0.0270, loss_G: 0.6145\n",
      "loss_GAN_A2B: 0.6043, loss_GAN_B2A: 0.3812, loss_cycle_ABA: 0.3361, loss_cycle_BAB: 0.6491, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.6984, loss_pix_B: 0.5782\n",
      "Epoch [8/10], Batch [1701/1749], loss_D: 0.0204, loss_G: 0.9195\n",
      "loss_GAN_A2B: 0.8902, loss_GAN_B2A: 0.7500, loss_cycle_ABA: 0.4402, loss_cycle_BAB: 0.3749, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.7822, loss_pix_B: 0.4803\n",
      "Epoch [9/10], Batch [1/1749], loss_D: 0.0211, loss_G: 0.8998\n",
      "loss_GAN_A2B: 1.0274, loss_GAN_B2A: 0.6036, loss_cycle_ABA: 0.1979, loss_cycle_BAB: 0.4585, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.6385, loss_pix_B: 0.5490\n",
      "Epoch [9/10], Batch [101/1749], loss_D: 0.0443, loss_G: 0.4612\n",
      "loss_GAN_A2B: 0.3994, loss_GAN_B2A: 0.3828, loss_cycle_ABA: 0.1960, loss_cycle_BAB: 0.3329, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.3978, loss_pix_B: 0.3789\n",
      "Epoch [9/10], Batch [201/1749], loss_D: 0.1116, loss_G: 0.8415\n",
      "loss_GAN_A2B: 0.7305, loss_GAN_B2A: 0.7629, loss_cycle_ABA: 0.2643, loss_cycle_BAB: 0.4539, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.9572, loss_pix_B: 0.5199\n",
      "Epoch [9/10], Batch [301/1749], loss_D: 0.0045, loss_G: 0.6308\n",
      "loss_GAN_A2B: 0.5038, loss_GAN_B2A: 0.5909, loss_cycle_ABA: 0.2070, loss_cycle_BAB: 0.4488, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.4624, loss_pix_B: 0.5089\n",
      "Epoch [9/10], Batch [401/1749], loss_D: 0.0071, loss_G: 0.7255\n",
      "loss_GAN_A2B: 0.8457, loss_GAN_B2A: 0.4400, loss_cycle_ABA: 0.2730, loss_cycle_BAB: 0.3508, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.5277, loss_pix_B: 0.5353\n",
      "Epoch [9/10], Batch [501/1749], loss_D: 0.1595, loss_G: 0.5089\n",
      "loss_GAN_A2B: 0.5131, loss_GAN_B2A: 0.2930, loss_cycle_ABA: 0.4518, loss_cycle_BAB: 0.4118, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.4403, loss_pix_B: 0.6939\n",
      "Epoch [9/10], Batch [601/1749], loss_D: 0.0159, loss_G: 0.7212\n",
      "loss_GAN_A2B: 1.0068, loss_GAN_B2A: 0.2779, loss_cycle_ABA: 0.1670, loss_cycle_BAB: 0.4465, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.6178, loss_pix_B: 0.5087\n",
      "Epoch [9/10], Batch [701/1749], loss_D: 0.1011, loss_G: 0.7297\n",
      "loss_GAN_A2B: 0.4774, loss_GAN_B2A: 0.8265, loss_cycle_ABA: 0.2450, loss_cycle_BAB: 0.3836, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.4174, loss_pix_B: 0.6567\n",
      "Epoch [9/10], Batch [801/1749], loss_D: 0.0771, loss_G: 0.6765\n",
      "loss_GAN_A2B: 0.9007, loss_GAN_B2A: 0.2977, loss_cycle_ABA: 0.1753, loss_cycle_BAB: 0.4404, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.4442, loss_pix_B: 0.6378\n",
      "Epoch [9/10], Batch [901/1749], loss_D: 0.0260, loss_G: 1.0998\n",
      "loss_GAN_A2B: 0.8173, loss_GAN_B2A: 1.1943, loss_cycle_ABA: 0.3442, loss_cycle_BAB: 0.4306, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.5730, loss_pix_B: 0.4774\n",
      "Epoch [9/10], Batch [1001/1749], loss_D: 0.1928, loss_G: 0.3310\n",
      "loss_GAN_A2B: 0.3367, loss_GAN_B2A: 0.1505, loss_cycle_ABA: 0.1760, loss_cycle_BAB: 0.4853, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.5980, loss_pix_B: 0.4714\n",
      "Epoch [9/10], Batch [1101/1749], loss_D: 0.0344, loss_G: 1.4338\n",
      "loss_GAN_A2B: 1.6106, loss_GAN_B2A: 1.0059, loss_cycle_ABA: 0.3254, loss_cycle_BAB: 0.7793, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.7666, loss_pix_B: 0.6091\n",
      "Epoch [9/10], Batch [1201/1749], loss_D: 0.0264, loss_G: 1.2855\n",
      "loss_GAN_A2B: 1.1510, loss_GAN_B2A: 1.2038, loss_cycle_ABA: 0.3547, loss_cycle_BAB: 0.5733, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.7323, loss_pix_B: 0.4998\n",
      "Epoch [9/10], Batch [1301/1749], loss_D: 0.0548, loss_G: 0.5837\n",
      "loss_GAN_A2B: 0.6499, loss_GAN_B2A: 0.2880, loss_cycle_ABA: 0.4388, loss_cycle_BAB: 0.4849, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.6066, loss_pix_B: 0.6156\n",
      "Epoch [9/10], Batch [1401/1749], loss_D: 0.0152, loss_G: 1.0171\n",
      "loss_GAN_A2B: 1.0905, loss_GAN_B2A: 0.7735, loss_cycle_ABA: 0.1684, loss_cycle_BAB: 0.5403, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.5803, loss_pix_B: 0.7390\n",
      "Epoch [9/10], Batch [1501/1749], loss_D: 0.0271, loss_G: 0.5254\n",
      "loss_GAN_A2B: 0.3137, loss_GAN_B2A: 0.5305, loss_cycle_ABA: 0.3382, loss_cycle_BAB: 0.4737, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.6013, loss_pix_B: 0.5600\n",
      "Epoch [9/10], Batch [1601/1749], loss_D: 0.0327, loss_G: 0.4570\n",
      "loss_GAN_A2B: 0.4674, loss_GAN_B2A: 0.2429, loss_cycle_ABA: 0.1948, loss_cycle_BAB: 0.5713, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.7041, loss_pix_B: 0.5816\n",
      "Epoch [9/10], Batch [1701/1749], loss_D: 0.0191, loss_G: 0.6687\n",
      "loss_GAN_A2B: 0.4659, loss_GAN_B2A: 0.6716, loss_cycle_ABA: 0.3185, loss_cycle_BAB: 0.4758, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.6359, loss_pix_B: 0.5322\n",
      "Epoch [10/10], Batch [1/1749], loss_D: 0.0218, loss_G: 0.9635\n",
      "loss_GAN_A2B: 0.8951, loss_GAN_B2A: 0.7796, loss_cycle_ABA: 0.4371, loss_cycle_BAB: 0.5727, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.6717, loss_pix_B: 0.4642\n",
      "Epoch [10/10], Batch [101/1749], loss_D: 0.0733, loss_G: 0.7902\n",
      "loss_GAN_A2B: 1.0069, loss_GAN_B2A: 0.3441, loss_cycle_ABA: 0.3462, loss_cycle_BAB: 0.6225, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.5302, loss_pix_B: 0.3367\n",
      "Epoch [10/10], Batch [201/1749], loss_D: 0.0385, loss_G: 0.7269\n",
      "loss_GAN_A2B: 0.5772, loss_GAN_B2A: 0.6655, loss_cycle_ABA: 0.2783, loss_cycle_BAB: 0.5268, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.6463, loss_pix_B: 0.5695\n",
      "Epoch [10/10], Batch [301/1749], loss_D: 0.0462, loss_G: 0.6459\n",
      "loss_GAN_A2B: 0.4672, loss_GAN_B2A: 0.6673, loss_cycle_ABA: 0.2305, loss_cycle_BAB: 0.3891, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.4475, loss_pix_B: 0.4352\n",
      "Epoch [10/10], Batch [401/1749], loss_D: 0.1115, loss_G: 0.6798\n",
      "loss_GAN_A2B: 0.7915, loss_GAN_B2A: 0.3817, loss_cycle_ABA: 0.2443, loss_cycle_BAB: 0.4567, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.6139, loss_pix_B: 0.4576\n",
      "Epoch [10/10], Batch [501/1749], loss_D: 0.0928, loss_G: 0.7359\n",
      "loss_GAN_A2B: 0.8026, loss_GAN_B2A: 0.4947, loss_cycle_ABA: 0.2901, loss_cycle_BAB: 0.4097, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.6244, loss_pix_B: 0.4482\n",
      "Epoch [10/10], Batch [601/1749], loss_D: 0.0637, loss_G: 0.6421\n",
      "loss_GAN_A2B: 0.5997, loss_GAN_B2A: 0.5313, loss_cycle_ABA: 0.1276, loss_cycle_BAB: 0.4636, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.6352, loss_pix_B: 0.4281\n",
      "Epoch [10/10], Batch [701/1749], loss_D: 0.1351, loss_G: 0.4548\n",
      "loss_GAN_A2B: 0.2567, loss_GAN_B2A: 0.5005, loss_cycle_ABA: 0.1627, loss_cycle_BAB: 0.3915, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.4679, loss_pix_B: 0.4163\n",
      "Epoch [10/10], Batch [801/1749], loss_D: 0.0509, loss_G: 0.8515\n",
      "loss_GAN_A2B: 0.5153, loss_GAN_B2A: 0.9289, loss_cycle_ABA: 0.5276, loss_cycle_BAB: 0.5652, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.9913, loss_pix_B: 0.4735\n",
      "Epoch [10/10], Batch [901/1749], loss_D: 0.0248, loss_G: 0.6681\n",
      "loss_GAN_A2B: 0.4684, loss_GAN_B2A: 0.6400, loss_cycle_ABA: 0.3298, loss_cycle_BAB: 0.5791, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.7420, loss_pix_B: 0.3369\n",
      "Epoch [10/10], Batch [1001/1749], loss_D: 0.0767, loss_G: 0.8442\n",
      "loss_GAN_A2B: 1.2626, loss_GAN_B2A: 0.2579, loss_cycle_ABA: 0.2358, loss_cycle_BAB: 0.4009, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.5650, loss_pix_B: 0.4831\n",
      "Epoch [10/10], Batch [1101/1749], loss_D: 0.0453, loss_G: 0.4594\n",
      "loss_GAN_A2B: 0.2250, loss_GAN_B2A: 0.4943, loss_cycle_ABA: 0.2417, loss_cycle_BAB: 0.5181, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.6380, loss_pix_B: 0.4633\n",
      "Epoch [10/10], Batch [1201/1749], loss_D: 0.0446, loss_G: 0.6213\n",
      "loss_GAN_A2B: 0.3745, loss_GAN_B2A: 0.6601, loss_cycle_ABA: 0.2943, loss_cycle_BAB: 0.5522, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.6808, loss_pix_B: 0.3654\n",
      "Epoch [10/10], Batch [1301/1749], loss_D: 0.0338, loss_G: 0.4974\n",
      "loss_GAN_A2B: 0.4855, loss_GAN_B2A: 0.3112, loss_cycle_ABA: 0.3173, loss_cycle_BAB: 0.4863, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.6487, loss_pix_B: 0.4512\n",
      "Epoch [10/10], Batch [1401/1749], loss_D: 0.0239, loss_G: 0.7871\n",
      "loss_GAN_A2B: 0.7711, loss_GAN_B2A: 0.6415, loss_cycle_ABA: 0.2190, loss_cycle_BAB: 0.4061, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.6662, loss_pix_B: 0.6681\n",
      "Epoch [10/10], Batch [1501/1749], loss_D: 0.1034, loss_G: 0.6731\n",
      "loss_GAN_A2B: 0.7674, loss_GAN_B2A: 0.3978, loss_cycle_ABA: 0.2169, loss_cycle_BAB: 0.4857, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.6867, loss_pix_B: 0.5141\n",
      "Epoch [10/10], Batch [1601/1749], loss_D: 0.0284, loss_G: 0.8008\n",
      "loss_GAN_A2B: 0.9676, loss_GAN_B2A: 0.4473, loss_cycle_ABA: 0.3776, loss_cycle_BAB: 0.3675, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.4952, loss_pix_B: 0.5348\n",
      "Epoch [10/10], Batch [1701/1749], loss_D: 0.0202, loss_G: 1.0169\n",
      "loss_GAN_A2B: 0.8354, loss_GAN_B2A: 1.0216, loss_cycle_ABA: 0.1924, loss_cycle_BAB: 0.5254, loss_identity_A: 0.0000, loss_identity_B: 0.0000, loss_pix_A: 0.5521, loss_pix_B: 0.4391\n"
     ]
    }
   ],
   "source": [
    "###### Training ######\n",
    "for epoch in range(epoch, n_epochs):\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        # Set model input\n",
    "        real_A = Variable(input_A.copy_(batch['A']))\n",
    "        real_B = Variable(input_B.copy_(batch['B']))\n",
    "\n",
    "        ###### Generators A2B and B2A ######\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        # Identity loss\n",
    "        # G_A2B(B) should equal B if real B is fed\n",
    "        same_B, _, _ = netG_A2B(real_B)\n",
    "        loss_identity_B = criterion_identity(same_B, real_B)*lambda_identity\n",
    "        # G_B2A(A) should equal A if real A is fed\n",
    "        same_A, _, _ = netG_B2A(real_A)\n",
    "        loss_identity_A = criterion_identity(same_A, real_A)*lambda_identity\n",
    "\n",
    "        # GAN loss\n",
    "        fake_B, mask_B, temp_B = netG_A2B(real_A)\n",
    "        recovered_A, _, _ = netG_B2A(fake_B)\n",
    "        pred_fake_B = netD_B(fake_B)\n",
    "\n",
    "        loss_cycle_ABA = criterion_cycle(recovered_A, real_A)\n",
    "        loss_GAN_A2B = criterion_GAN(pred_fake_B, target_real)\n",
    "        loss_pix_A = criterion_identity(fake_B, real_A)\n",
    "\n",
    "        fake_A, mask_A, temp_A = netG_B2A(real_B)\n",
    "        recovered_B, _, _  = netG_A2B(fake_A)\n",
    "        pred_fake_A = netD_A(fake_A)\n",
    "\n",
    "        loss_cycle_BAB = criterion_cycle(recovered_B, real_B)\n",
    "        loss_GAN_B2A = criterion_GAN(pred_fake_A, target_real)\n",
    "        loss_pix_B = criterion_identity(fake_A, real_B)\n",
    "\n",
    "        loss_reg_A = lambda_reg * (\n",
    "                torch.sum(torch.abs(mask_A[:, :, :, :-1] - mask_A[:, :, :, 1:])) +\n",
    "                torch.sum(torch.abs(mask_A[:, :, :-1, :] - mask_A[:, :, 1:, :])))\n",
    "\n",
    "        loss_reg_B = lambda_reg * (\n",
    "                torch.sum(torch.abs(mask_B[:, :, :, :-1] - mask_B[:, :, :, 1:])) +\n",
    "                torch.sum(torch.abs(mask_B[:, :, :-1, :] - mask_B[:, :, 1:, :])))\n",
    "\n",
    "        # Total loss\n",
    "        if epoch < gan_curriculum:\n",
    "            rate = starting_rate\n",
    "            # print('using curriculum gan')\n",
    "        else:\n",
    "            rate = default_rate\n",
    "            # print('using normal gan')\n",
    "\n",
    "        loss_G = ((loss_GAN_A2B + loss_GAN_B2A)*0.5 + (loss_reg_A + loss_reg_B))* (1.-rate) + ((loss_cycle_ABA + loss_cycle_BAB)*lambda_cycle+(loss_pix_B+loss_pix_A)*lambda_pixel)* rate\n",
    "\n",
    "        loss_G.backward()\n",
    "        optimizer_G.step()\n",
    "        ###################################\n",
    "\n",
    "        optimizer_D.zero_grad()\n",
    "\n",
    "        # Real loss\n",
    "        pred_real_A = netD_A.forward(real_A)\n",
    "        loss_D_real_A = criterion_GAN(pred_real_A, target_real)\n",
    "\n",
    "        # Fake loss\n",
    "        fake_A = fake_A_buffer.push_and_pop(fake_A)\n",
    "        pred_fake_A = netD_A.forward(fake_A.detach())\n",
    "        loss_D_fake_A = criterion_GAN(pred_fake_A, target_fake)\n",
    "\n",
    "        # Real loss\n",
    "        pred_real_B = netD_B.forward(real_B)\n",
    "        loss_D_real_B = criterion_GAN(pred_real_B, target_real)\n",
    "\n",
    "        # Fake loss\n",
    "        fake_B = fake_B_buffer.push_and_pop(fake_B)\n",
    "        pred_fake_B = netD_B.forward(fake_B.detach())\n",
    "        loss_D_fake_B = criterion_GAN(pred_fake_B, target_fake)\n",
    "\n",
    "        # Total loss\n",
    "        loss_D = (loss_D_real_B + loss_D_fake_B + loss_D_real_A + loss_D_fake_A)*0.25\n",
    "\n",
    "        loss_D.backward()\n",
    "        optimizer_D.step()\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            print('Epoch [%d/%d], Batch [%d/%d], loss_D: %.4f, loss_G: %.4f' % (epoch+1, n_epochs,i+1, len(dataloader), loss_D.item(), loss_G.item()))\n",
    "            print('loss_GAN_A2B: %.4f, loss_GAN_B2A: %.4f, loss_cycle_ABA: %.4f, loss_cycle_BAB: %.4f, loss_identity_A: %.4f, loss_identity_B: %.4f, loss_pix_A: %.4f, loss_pix_B: %.4f' % (loss_GAN_A2B.item(),\n",
    "                loss_GAN_B2A.item(), loss_cycle_ABA.item(), loss_cycle_BAB.item(), loss_identity_A.item(), loss_identity_B.item(), loss_pix_A.item(), loss_pix_B.item()))\n",
    "\n",
    "        wandb.log(\n",
    "            {\n",
    "                'Discriminator Loss': loss_D.item(),\n",
    "                'Generator Loss': loss_G.item(),\n",
    "                'CT to MR Loss': loss_GAN_A2B.item(),\n",
    "                'MR to CT Loss': loss_GAN_B2A.item(),\n",
    "                'CT Reconstruction Loss': loss_cycle_ABA.item(),\n",
    "                'MR Reconstruction Loss': loss_cycle_BAB.item(),\n",
    "                'CT Identity Loss': loss_identity_A.item(),\n",
    "                'MR Identity Loss': loss_identity_B.item(),\n",
    "                'CT Pixel-Wise Reconstruction Loss': loss_pix_A.item(),\n",
    "                'MR Pixel-Wise Reconstruction Loss': loss_pix_B.item(),\n",
    "            }\n",
    "        )\n",
    "\n",
    "        wandb.log(\n",
    "            {\n",
    "                'Structural Similarity Index Measure (SSIM) - CT to MR': ssi(real_A.data.cpu(), fake_A.data.cpu()),\n",
    "                'Peak Signal-to-Noise Ratio (PSNR) - CT to MR': psnr(real_A.data.cpu(), fake_A.data.cpu()),\n",
    "                'Universal Quality Index (UQI) - CT to MR': uqi(real_A.data.cpu(), fake_A.data.cpu()),\n",
    "                'Visual Information Fidelity (VIF) - CT to MR': vif(real_A.data.cpu(), fake_A.data.cpu()),\n",
    "                'Structural Similarity Index Measure (SSIM) - MR to CT': ssi(real_B.data.cpu(), fake_B.data.cpu()),\n",
    "                'Peak Signal-to-Noise Ratio (PSNR) - MR to CT': psnr(real_B.data.cpu(), fake_B.data.cpu()),\n",
    "                'Universal Quality Index (UQI) - MR to CT': uqi(real_B.data.cpu(), fake_B.data.cpu()),\n",
    "                'Visual Information Fidelity (VIF) - MR to CT': vif(real_B.data.cpu(), fake_B.data.cpu())\n",
    "            }\n",
    "        )\n",
    "\n",
    "        save_path='%s/%s' % (save_name, 'training')\n",
    "        if not os.path.exists(save_path):\n",
    "            os.makedirs(save_path)\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            image_array = torch.cat([\n",
    "                real_A.data.cpu()[0]* 0.5 + 0.5,\n",
    "                mask_B.data.cpu()[0],\n",
    "                fake_B.data.cpu()[0]* 0.5+0.5,\n",
    "                temp_B.data.cpu()[0]* 0.5+0.5,\n",
    "                real_B.data.cpu()[0]* 0.5 + 0.5,\n",
    "                mask_A.data.cpu()[0],\n",
    "                fake_A.data.cpu()[0]* 0.5+0.5,\n",
    "                temp_A.data.cpu()[0]* 0.5+0.5],\n",
    "                2)\n",
    "            wandb.log({\n",
    "                \"examples\": wandb.Image(image_array, caption=f\"Epoch {epoch} (Batch {i}) Real CT, Masked MR, Fake MR, Temp MR, Real MR, Masked CT, Fake CT, Temp CT\")\n",
    "                })\n",
    "\n",
    "        # image_array = torch.cat([\n",
    "        #     real_B.data.cpu()[0], # * 0.5 + 0.5,\n",
    "        #     mask_A.data.cpu()[0],\n",
    "        #     fake_A.data.cpu()[0], #*0.5+0.5,\n",
    "        #     temp_A.data.cpu()[0]], #*0.5+0.5],\n",
    "        #     2)\n",
    "\n",
    "        # wandb.log({\n",
    "        #     \"examples\": wandb.Image(image_array, caption=\"Real MR, Masked CT, Fake CT, Temp CT\")\n",
    "        #     })\n",
    "\n",
    "        # save_image(torch.cat([\n",
    "        #     real_A.data.cpu()[0] * 0.5 + 0.5,\n",
    "        #     mask_B.data.cpu()[0],\n",
    "        #     fake_B.data.cpu()[0]*0.5+0.5, temp_B.data.cpu()[0]*0.5+0.5], 2),\n",
    "        #     '%s/%04d_%04d_progress_B.png' % (save_path,epoch+1,i+1))\n",
    "\n",
    "        # save_image(torch.cat([\n",
    "        #     real_B.data.cpu()[0] * 0.5 + 0.5,\n",
    "        #     mask_A.data.cpu()[0],\n",
    "        #     fake_A.data.cpu()[0]*0.5+0.5, temp_A.data.cpu()[0]*0.5+0.5], 2),\n",
    "        #     '%s/%04d_%04d_progress_A.png' % (save_path,epoch+1,i+1))\n",
    "\n",
    "\n",
    "    # Update learning rates\n",
    "    lr_scheduler_G.step()\n",
    "    lr_scheduler_D.step()\n",
    "\n",
    "    torch.save(netG_A2B.state_dict(), '%s/%s' % (save_name, 'netG_A2B.pth'))\n",
    "    torch.save(netG_B2A.state_dict(), '%s/%s' % (save_name, 'netG_B2A.pth'))\n",
    "    torch.save(netD_A.state_dict(), '%s/%s' % (save_name, 'netD_A.pth'))\n",
    "    torch.save(netD_B.state_dict(), '%s/%s' % (save_name, 'netD_B.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mvpP8aYoa7gR"
   },
   "source": [
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "id": "iTRmZwPFa7Ba"
   },
   "outputs": [],
   "source": [
    "batch_size = 1 #size of the batches')\n",
    "save_name = 'MedAttentionGAN_2_no_clip'\n",
    "dataroot = 'juh_mr_ct' #root directory of the dataset\n",
    "input_nc = 3 #number of channels of input data'\n",
    "output_nc = 3 #number of channels of output data'\n",
    "size = 256 #size of the data (squared assumed)'\n",
    "cuda = 'store_true', #use GPU computation'\n",
    "n_cpu = 8 #number of cpu threads to use during batch generation'\n",
    "\n",
    "if torch.cuda.is_available() and not cuda:\n",
    "    print(\"WARNING: You have a CUDA device, so you should probably run with --cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7M5p71o4cCnb",
    "outputId": "5d868f3f-db3d-48fa-fe2a-be4bfb80856f",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generator(\n",
       "  (model): Sequential(\n",
       "    (0): ReflectionPad2d((3, 3, 3, 3))\n",
       "    (1): Conv2d(3, 64, kernel_size=(7, 7), stride=(1, 1))\n",
       "    (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (8): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (9): ReLU(inplace=True)\n",
       "    (10): ResnetBlock(\n",
       "      (conv_block): Sequential(\n",
       "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "        (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): ReLU(inplace=True)\n",
       "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "        (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (11): ResnetBlock(\n",
       "      (conv_block): Sequential(\n",
       "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "        (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): ReLU(inplace=True)\n",
       "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "        (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (12): ResnetBlock(\n",
       "      (conv_block): Sequential(\n",
       "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "        (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): ReLU(inplace=True)\n",
       "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "        (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (13): ResnetBlock(\n",
       "      (conv_block): Sequential(\n",
       "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "        (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): ReLU(inplace=True)\n",
       "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "        (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (14): ResnetBlock(\n",
       "      (conv_block): Sequential(\n",
       "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "        (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): ReLU(inplace=True)\n",
       "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "        (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (15): ResnetBlock(\n",
       "      (conv_block): Sequential(\n",
       "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "        (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): ReLU(inplace=True)\n",
       "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "        (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (16): ReflectionPad2d((1, 1, 1, 1))\n",
       "    (17): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (18): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (19): ReLU(inplace=True)\n",
       "    (20): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (21): PixelShuffle(upscale_factor=2)\n",
       "    (22): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (23): ReLU(inplace=True)\n",
       "    (24): ReflectionPad2d((1, 1, 1, 1))\n",
       "    (25): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (26): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (27): ReLU(inplace=True)\n",
       "    (28): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (29): PixelShuffle(upscale_factor=2)\n",
       "    (30): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (31): ReLU(inplace=True)\n",
       "    (32): ReflectionPad2d((3, 3, 3, 3))\n",
       "    (33): Conv2d(64, 4, kernel_size=(7, 7), stride=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###### Definition of variables ######\n",
    "# Networks\n",
    "netG_A2B = Generator()\n",
    "netG_B2A = Generator()\n",
    "\n",
    "if cuda:\n",
    "    netG_A2B.cuda()\n",
    "    netG_B2A.cuda()\n",
    "\n",
    "# Load state dicts\n",
    "netG_A2B.load_state_dict(torch.load('%s/%s' % (save_name, 'netG_A2B.pth')))\n",
    "netG_B2A.load_state_dict(torch.load('%s/%s' % (save_name, 'netG_B2A.pth')))\n",
    "\n",
    "# Set model's test mode\n",
    "netG_A2B.eval()\n",
    "netG_B2A.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 366
    },
    "id": "zkGTo1-vcGBT",
    "outputId": "102326bd-5a52-4a9f-f46c-bbdf8c1ca101"
   },
   "outputs": [],
   "source": [
    "# Inputs & targets memory allocation\n",
    "Tensor = torch.cuda.FloatTensor if cuda else torch.Tensor\n",
    "input_A = Tensor(batch_size, input_nc, size, size)\n",
    "input_B = Tensor(batch_size, output_nc, size, size)\n",
    "\n",
    "# Dataset loader\n",
    "transforms_ = [GrayscaleToRGB(),\n",
    "               transforms.Resize((256,256)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "                transforms.ToTensor(),\n",
    "              transforms.Normalize([0.5,0.5,0.5], [0.5,0.5,0.5])]\n",
    "dataloader = DataLoader(ImageDataset(dataroot, transforms_=transforms_, mode='test'),\n",
    "                        batch_size=batch_size, shuffle=False, num_workers=n_cpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:hxtmqzyq) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "527b6f61ba6e4e6b8cb981d4dbde7ccc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='20.101 MB of 20.101 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Peak Signal-to-Noise Ratio (PSNR) - CT to MR</td><td>▆▅▇▇▆▆▇▄▅▆▅▃▅▅▄▆▇▃▅▇▅▅▄▆▇▅▆▇▇▅█▆▅▃▃▁█▅▅▄</td></tr><tr><td>Peak Signal-to-Noise Ratio (PSNR) - MR to CT</td><td>█▂▂▃▁▄▂▆▄▁▂▁▂▅▂▂▃▄▃▅▄▆▅▃▂▂▂▃▂▄▃▂▆▂▃▂▂▅▃▂</td></tr><tr><td>Structural Similarity Index Measure (SSIM) - CT to MR</td><td>▄▄▃▂▃▃▃▅▂▃▂▃▃▄▆▄▄▂▃▅▅▅▄▃▅▂▁▃▃▅█▃▁▄▅▁█▃▂▄</td></tr><tr><td>Structural Similarity Index Measure (SSIM) - MR to CT</td><td>▇▃▂▂▁▃▃█▂▃▁▅▂▅▆▃▄▅▄▅▆▆▃▄▄▄▂▃▂▆▇▂▃▃▇▄▇▄▄▄</td></tr><tr><td>Universal Quality Index (UQI) - CT to MR</td><td>▇▃▂▃▂▆▃▅▃▄▃▇▁▃▃▄▃▅▃▂▁▂▃▁▃█▁▁▃▄▆▂▅█▄▄▄▁▄▃</td></tr><tr><td>Universal Quality Index (UQI) - MR to CT</td><td>▅▁▅▂▄▇▃▅▁▅▅█▄▃▄▃▅▅▅▇▃▃▃▃▄▃▅▂▅▄▆▄▄▃▄▅▆▃▁▃</td></tr><tr><td>Visual Information Fidelity (VIF) - CT to MR</td><td>▃▂▂▂▃▇▂▅▂▂▁▅▁▂▄▂▂▅▂▃▅▂▂▁▃▄▁▂▁▂█▃▂▄▂▄▄▃▂▂</td></tr><tr><td>Visual Information Fidelity (VIF) - MR to CT</td><td>▄▂▂▂▄▄▃▂▂▆▃▂▃▂▃▂▂▂▂█▃▁▂▂▂▃▄▂▂▂▄▃▂▂▁▂▃▂▃▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Peak Signal-to-Noise Ratio (PSNR) - CT to MR</td><td>14.47996</td></tr><tr><td>Peak Signal-to-Noise Ratio (PSNR) - MR to CT</td><td>15.17131</td></tr><tr><td>Structural Similarity Index Measure (SSIM) - CT to MR</td><td>0.38029</td></tr><tr><td>Structural Similarity Index Measure (SSIM) - MR to CT</td><td>0.40261</td></tr><tr><td>Universal Quality Index (UQI) - CT to MR</td><td>0.00157</td></tr><tr><td>Universal Quality Index (UQI) - MR to CT</td><td>0.00097</td></tr><tr><td>Visual Information Fidelity (VIF) - CT to MR</td><td>0.18996</td></tr><tr><td>Visual Information Fidelity (VIF) - MR to CT</td><td>0.02047</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">swept-spaceship-30</strong> at: <a href='https://wandb.ai/thedlproject/deep_learning_project/runs/hxtmqzyq' target=\"_blank\">https://wandb.ai/thedlproject/deep_learning_project/runs/hxtmqzyq</a><br/> View job at <a href='https://wandb.ai/thedlproject/deep_learning_project/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjEyNjkwOTYzMA==/version_details/v3' target=\"_blank\">https://wandb.ai/thedlproject/deep_learning_project/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjEyNjkwOTYzMA==/version_details/v3</a><br/>Synced 6 W&B file(s), 90 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240103_200123-hxtmqzyq/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:hxtmqzyq). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad189b3df1844d3092e3a4588284727e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016669802639322977, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/ibex/user/sabbam0a/template/dlp/wandb/run-20240103_200316-r1cht6x9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thedlproject/deep_learning_project/runs/r1cht6x9' target=\"_blank\">eager-puddle-31</a></strong> to <a href='https://wandb.ai/thedlproject/deep_learning_project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thedlproject/deep_learning_project' target=\"_blank\">https://wandb.ai/thedlproject/deep_learning_project</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thedlproject/deep_learning_project/runs/r1cht6x9' target=\"_blank\">https://wandb.ai/thedlproject/deep_learning_project/runs/r1cht6x9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/thedlproject/deep_learning_project/runs/r1cht6x9?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x147d44f265e0>"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project='deep_learning_project',\n",
    "    entity='thedlproject'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "jpv59m9qcMAU",
    "outputId": "ccb6ed5c-e88e-4f46-edb0-7a81d6477f07"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ibex/user/sabbam0a/template/env/lib/python3.8/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 6, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate(dataloader):\n",
    "    # Set model input\n",
    "    real_A = Variable(input_A.copy_(batch['A']))\n",
    "    real_B = Variable(input_B.copy_(batch['B']))\n",
    "\n",
    "    # Generate output\n",
    "    fake_B, mask_B, temp_B = netG_A2B(real_A)\n",
    "    # fake_B_1 = 0.5*fake_B.data[0] + 0.5\n",
    "    # fake_B_2 = 0.5*temp_B.data[0] + 0.5\n",
    "    fake_A, mask_A, temp_A = netG_B2A(real_B)\n",
    "#     fake_A_1 = 0.5*fake_A.data[0] + 0.5\n",
    "#     fake_A_2 = 0.5*temp_A.data[0] + 0.5\n",
    "    \n",
    "    \n",
    "    image_array = torch.cat([\n",
    "                real_A.data.cpu()[0]* 0.5 + 0.5,\n",
    "                mask_B.data.cpu()[0],\n",
    "                fake_B.data.cpu()[0]* 0.5+0.5,\n",
    "                temp_B.data.cpu()[0]* 0.5+0.5,\n",
    "                real_B.data.cpu()[0]* 0.5 + 0.5,\n",
    "                mask_A.data.cpu()[0],\n",
    "                fake_A.data.cpu()[0]* 0.5+0.5,\n",
    "                temp_A.data.cpu()[0]* 0.5+0.5],\n",
    "                2)\n",
    "    wandb.log({\n",
    "        \"examples\": wandb.Image(image_array, caption=f\"TEST (paired): Real CT, Masked MR, Fake MR, Temp MR, Real MR, Masked CT, Fake CT, Temp CT\")\n",
    "        })\n",
    "    wandb.log(\n",
    "            {\n",
    "                'Structural Similarity Index Measure (SSIM) - CT to MR': ssi(real_A.data.cpu(), fake_A.data.cpu()),\n",
    "                'Peak Signal-to-Noise Ratio (PSNR) - CT to MR': psnr(real_A.data.cpu(), fake_A.data.cpu()),\n",
    "                'Universal Quality Index (UQI) - CT to MR': uqi(real_A.data.cpu(), fake_A.data.cpu()),\n",
    "                'Visual Information Fidelity (VIF) - CT to MR': vif(real_A.data.cpu(), fake_A.data.cpu()),\n",
    "                'Structural Similarity Index Measure (SSIM) - MR to CT': ssi(real_B.data.cpu(), fake_B.data.cpu()),\n",
    "                'Peak Signal-to-Noise Ratio (PSNR) - MR to CT': psnr(real_B.data.cpu(), fake_B.data.cpu()),\n",
    "                'Universal Quality Index (UQI) - MR to CT': uqi(real_B.data.cpu(), fake_B.data.cpu()),\n",
    "                'Visual Information Fidelity (VIF) - MR to CT': vif(real_B.data.cpu(), fake_B.data.cpu())\n",
    "            }\n",
    "        )\n",
    "    # # Show images\n",
    "    # imshow(real_A.data.cpu()[0]*0.5+0.5, text='REAL CT', should_save=False)\n",
    "    # imshow(real_B.data.cpu()[0]*0.5+0.5, text='REAL MR', should_save=False)\n",
    "    # imshow(fake_A_1, text='FAKE CT (WITH MASKING)', should_save=False)\n",
    "    # imshow(fake_B_1, text='FAKE MR (WITH MASKING)', should_save=False)\n",
    "    # imshow(fake_A_2, text='FAKE CT (WITHOUT MASKING)', should_save=False)\n",
    "    # imshow(fake_B_2, text='FAKE MR (WITHOUT MASKING)', should_save=False)\n",
    "    # imshow(mask_A.data.cpu()[0], text='CT MASK)', should_save=False)\n",
    "    # imshow(mask_B.data.cpu()[0], text='MR MASK)', should_save=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "200272c1e0244cf1bac778dc4e2ecc84": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "29a6839f6f7b4e2dae8669c903ad11e6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "578d1bcbf5aa45e5aeabbab2187bcfca": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b38c39c8506f4e3583e153b62d54aa47": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "LabelModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_29a6839f6f7b4e2dae8669c903ad11e6",
      "placeholder": "​",
      "style": "IPY_MODEL_578d1bcbf5aa45e5aeabbab2187bcfca",
      "value": "910.591 MB of 910.591 MB uploaded\r"
     }
    },
    "b519be0dc5e549a6a18937107657e530": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ba3a7b322c5b488c95ca9ababad6b547": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f1b489b72b564db9b2f557a11378df14",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_200272c1e0244cf1bac778dc4e2ecc84",
      "value": 1
     }
    },
    "bcad0ed52b784d6f8c045f987ac08837": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "VBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b38c39c8506f4e3583e153b62d54aa47",
       "IPY_MODEL_ba3a7b322c5b488c95ca9ababad6b547"
      ],
      "layout": "IPY_MODEL_b519be0dc5e549a6a18937107657e530"
     }
    },
    "f1b489b72b564db9b2f557a11378df14": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
